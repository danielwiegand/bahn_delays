21/07/04 20:38:13.282 main WARN Utils: Your hostname, Yoga resolves to a loopback address: 127.0.1.1; using 192.168.43.54 instead (on interface wlp4s0)
21/07/04 20:38:13.283 main WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/07/04 20:38:15.489 main WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/07/04 20:38:16.052 main INFO SecurityManager: Changing view acls to: daniel
21/07/04 20:38:16.054 main INFO SecurityManager: Changing modify acls to: daniel
21/07/04 20:38:16.057 main INFO SecurityManager: Changing view acls groups to: 
21/07/04 20:38:16.059 main INFO SecurityManager: Changing modify acls groups to: 
21/07/04 20:38:16.060 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/04 20:38:17.021 Thread-3 INFO SparkContext: Running Spark version 3.1.2
21/07/04 20:38:17.260 Thread-3 INFO ResourceUtils: ==============================================================
21/07/04 20:38:17.262 Thread-3 INFO ResourceUtils: No custom resources configured for spark.driver.
21/07/04 20:38:17.263 Thread-3 INFO ResourceUtils: ==============================================================
21/07/04 20:38:17.264 Thread-3 INFO SparkContext: Submitted application: myApp
21/07/04 20:38:17.446 Thread-3 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/07/04 20:38:17.484 Thread-3 INFO ResourceProfile: Limiting resource is cpu
21/07/04 20:38:17.486 Thread-3 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/07/04 20:38:17.749 Thread-3 INFO SecurityManager: Changing view acls to: daniel
21/07/04 20:38:17.756 Thread-3 INFO SecurityManager: Changing modify acls to: daniel
21/07/04 20:38:17.757 Thread-3 INFO SecurityManager: Changing view acls groups to: 
21/07/04 20:38:17.758 Thread-3 INFO SecurityManager: Changing modify acls groups to: 
21/07/04 20:38:17.760 Thread-3 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/04 20:38:19.056 Thread-3 INFO Utils: Successfully started service 'sparkDriver' on port 38357.
21/07/04 20:38:19.181 Thread-3 INFO SparkEnv: Registering MapOutputTracker
21/07/04 20:38:19.323 Thread-3 INFO SparkEnv: Registering BlockManagerMaster
21/07/04 20:38:19.446 Thread-3 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/04 20:38:19.448 Thread-3 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/04 20:38:19.466 Thread-3 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/04 20:38:19.560 Thread-3 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-44927a22-7edd-42ad-9ee5-526658759cf6
21/07/04 20:38:19.653 Thread-3 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
21/07/04 20:38:19.728 Thread-3 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/04 20:38:20.148 Thread-3 INFO log: Logging initialized @10918ms to org.sparkproject.jetty.util.log.Slf4jLog
21/07/04 20:38:20.359 Thread-3 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-Ubuntu-0ubuntu2.20.04
21/07/04 20:38:20.399 Thread-3 INFO Server: Started @11173ms
21/07/04 20:38:20.472 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
21/07/04 20:38:20.500 Thread-3 INFO AbstractConnector: Started ServerConnector@7e74d394{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}
21/07/04 20:38:20.502 Thread-3 INFO Utils: Successfully started service 'SparkUI' on port 4041.
21/07/04 20:38:20.582 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2385546a{/jobs,null,AVAILABLE,@Spark}
21/07/04 20:38:20.592 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@53f7be78{/jobs/json,null,AVAILABLE,@Spark}
21/07/04 20:38:20.594 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3ae6f2bd{/jobs/job,null,AVAILABLE,@Spark}
21/07/04 20:38:20.609 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1694575a{/jobs/job/json,null,AVAILABLE,@Spark}
21/07/04 20:38:20.610 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@101f9a6c{/stages,null,AVAILABLE,@Spark}
21/07/04 20:38:20.612 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@48fd4f9b{/stages/json,null,AVAILABLE,@Spark}
21/07/04 20:38:20.614 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@32a603f0{/stages/stage,null,AVAILABLE,@Spark}
21/07/04 20:38:20.625 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4901771{/stages/stage/json,null,AVAILABLE,@Spark}
21/07/04 20:38:20.627 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5f1f02b9{/stages/pool,null,AVAILABLE,@Spark}
21/07/04 20:38:20.630 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2ca30f0f{/stages/pool/json,null,AVAILABLE,@Spark}
21/07/04 20:38:20.632 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@32b24a9{/storage,null,AVAILABLE,@Spark}
21/07/04 20:38:20.639 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7a724429{/storage/json,null,AVAILABLE,@Spark}
21/07/04 20:38:20.642 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@33311064{/storage/rdd,null,AVAILABLE,@Spark}
21/07/04 20:38:20.645 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7aba3b14{/storage/rdd/json,null,AVAILABLE,@Spark}
21/07/04 20:38:20.648 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3683f01{/environment,null,AVAILABLE,@Spark}
21/07/04 20:38:20.650 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@310a1934{/environment/json,null,AVAILABLE,@Spark}
21/07/04 20:38:20.655 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@465ef909{/executors,null,AVAILABLE,@Spark}
21/07/04 20:38:20.658 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@222524b7{/executors/json,null,AVAILABLE,@Spark}
21/07/04 20:38:20.661 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@751d58cc{/executors/threadDump,null,AVAILABLE,@Spark}
21/07/04 20:38:20.664 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@75af802{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/07/04 20:38:20.687 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@f0ce398{/static,null,AVAILABLE,@Spark}
21/07/04 20:38:20.694 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4312f613{/,null,AVAILABLE,@Spark}
21/07/04 20:38:20.718 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4218f0d2{/api,null,AVAILABLE,@Spark}
21/07/04 20:38:20.721 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6bcb1d06{/jobs/job/kill,null,AVAILABLE,@Spark}
21/07/04 20:38:20.723 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1d79ce85{/stages/stage/kill,null,AVAILABLE,@Spark}
21/07/04 20:38:20.726 Thread-3 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.43.54:4041
21/07/04 20:38:21.671 Thread-3 INFO Executor: Starting executor ID driver on host 192.168.43.54
21/07/04 20:38:21.725 Thread-3 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40701.
21/07/04 20:38:21.726 Thread-3 INFO NettyBlockTransferService: Server created on 192.168.43.54:40701
21/07/04 20:38:21.729 Thread-3 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/04 20:38:21.756 Thread-3 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.43.54, 40701, None)
21/07/04 20:38:21.774 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager 192.168.43.54:40701 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.43.54, 40701, None)
21/07/04 20:38:21.781 Thread-3 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.43.54, 40701, None)
21/07/04 20:38:21.785 Thread-3 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.43.54, 40701, None)
21/07/04 20:38:22.404 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1e6e611{/metrics/json,null,AVAILABLE,@Spark}
21/07/04 20:38:23.386 Thread-3 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse').
21/07/04 20:38:23.389 Thread-3 INFO SharedState: Warehouse path is 'file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse'.
21/07/04 20:38:23.447 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@76db1232{/SQL,null,AVAILABLE,@Spark}
21/07/04 20:38:23.451 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@51254c7b{/SQL/json,null,AVAILABLE,@Spark}
21/07/04 20:38:23.455 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@25c1b8a0{/SQL/execution,null,AVAILABLE,@Spark}
21/07/04 20:38:23.457 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@57391092{/SQL/execution/json,null,AVAILABLE,@Spark}
21/07/04 20:38:23.490 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6c94c0c5{/static/sql,null,AVAILABLE,@Spark}
21/07/04 20:48:35.470 main WARN Utils: Your hostname, Yoga resolves to a loopback address: 127.0.1.1; using 192.168.43.54 instead (on interface wlp4s0)
21/07/04 20:48:35.480 main WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/07/04 20:48:37.478 main WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/07/04 20:48:38.005 main INFO SecurityManager: Changing view acls to: daniel
21/07/04 20:48:38.006 main INFO SecurityManager: Changing modify acls to: daniel
21/07/04 20:48:38.010 main INFO SecurityManager: Changing view acls groups to: 
21/07/04 20:48:38.017 main INFO SecurityManager: Changing modify acls groups to: 
21/07/04 20:48:38.019 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/04 20:48:39.179 Thread-3 INFO SparkContext: Running Spark version 3.1.2
21/07/04 20:48:39.492 Thread-3 INFO ResourceUtils: ==============================================================
21/07/04 20:48:39.494 Thread-3 INFO ResourceUtils: No custom resources configured for spark.driver.
21/07/04 20:48:39.496 Thread-3 INFO ResourceUtils: ==============================================================
21/07/04 20:48:39.497 Thread-3 INFO SparkContext: Submitted application: myApp
21/07/04 20:48:39.666 Thread-3 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/07/04 20:48:39.721 Thread-3 INFO ResourceProfile: Limiting resource is cpu
21/07/04 20:48:39.724 Thread-3 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/07/04 20:48:40.033 Thread-3 INFO SecurityManager: Changing view acls to: daniel
21/07/04 20:48:40.035 Thread-3 INFO SecurityManager: Changing modify acls to: daniel
21/07/04 20:48:40.036 Thread-3 INFO SecurityManager: Changing view acls groups to: 
21/07/04 20:48:40.037 Thread-3 INFO SecurityManager: Changing modify acls groups to: 
21/07/04 20:48:40.038 Thread-3 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/04 20:48:40.881 Thread-3 INFO Utils: Successfully started service 'sparkDriver' on port 41803.
21/07/04 20:48:40.951 Thread-3 INFO SparkEnv: Registering MapOutputTracker
21/07/04 20:48:41.019 Thread-3 INFO SparkEnv: Registering BlockManagerMaster
21/07/04 20:48:41.098 Thread-3 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/04 20:48:41.100 Thread-3 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/04 20:48:41.111 Thread-3 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/04 20:48:41.139 Thread-3 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-546d7e5f-b55b-4017-85e2-170775060a1f
21/07/04 20:48:41.185 Thread-3 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
21/07/04 20:48:41.223 Thread-3 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/04 20:48:41.461 Thread-3 INFO log: Logging initialized @11762ms to org.sparkproject.jetty.util.log.Slf4jLog
21/07/04 20:48:41.643 Thread-3 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-Ubuntu-0ubuntu2.20.04
21/07/04 20:48:41.726 Thread-3 INFO Server: Started @12030ms
21/07/04 20:48:41.807 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
21/07/04 20:48:41.809 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
21/07/04 20:48:41.834 Thread-3 INFO AbstractConnector: Started ServerConnector@1acd0a4{HTTP/1.1, (http/1.1)}{0.0.0.0:4042}
21/07/04 20:48:41.834 Thread-3 INFO Utils: Successfully started service 'SparkUI' on port 4042.
21/07/04 20:48:41.896 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7fc8f302{/jobs,null,AVAILABLE,@Spark}
21/07/04 20:48:41.900 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@922dbf8{/jobs/json,null,AVAILABLE,@Spark}
21/07/04 20:48:41.902 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@88e72a9{/jobs/job,null,AVAILABLE,@Spark}
21/07/04 20:48:41.910 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@281dfd12{/jobs/job/json,null,AVAILABLE,@Spark}
21/07/04 20:48:41.912 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4a3fada1{/stages,null,AVAILABLE,@Spark}
21/07/04 20:48:41.914 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@783200c6{/stages/json,null,AVAILABLE,@Spark}
21/07/04 20:48:41.915 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3f4d883f{/stages/stage,null,AVAILABLE,@Spark}
21/07/04 20:48:41.919 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7c6acb46{/stages/stage/json,null,AVAILABLE,@Spark}
21/07/04 20:48:41.920 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@54589733{/stages/pool,null,AVAILABLE,@Spark}
21/07/04 20:48:41.924 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@583624f5{/stages/pool/json,null,AVAILABLE,@Spark}
21/07/04 20:48:41.926 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@61a2f509{/storage,null,AVAILABLE,@Spark}
21/07/04 20:48:41.929 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1deb4742{/storage/json,null,AVAILABLE,@Spark}
21/07/04 20:48:41.931 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@285fb26{/storage/rdd,null,AVAILABLE,@Spark}
21/07/04 20:48:41.933 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2f4f4a4d{/storage/rdd/json,null,AVAILABLE,@Spark}
21/07/04 20:48:41.935 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3c69094d{/environment,null,AVAILABLE,@Spark}
21/07/04 20:48:41.937 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4f5428f6{/environment/json,null,AVAILABLE,@Spark}
21/07/04 20:48:41.940 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6e2a0878{/executors,null,AVAILABLE,@Spark}
21/07/04 20:48:41.942 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7a1762d0{/executors/json,null,AVAILABLE,@Spark}
21/07/04 20:48:41.944 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3405f9e2{/executors/threadDump,null,AVAILABLE,@Spark}
21/07/04 20:48:41.948 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3635d26e{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/07/04 20:48:41.964 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@35e6e164{/static,null,AVAILABLE,@Spark}
21/07/04 20:48:41.966 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@16ac0d71{/,null,AVAILABLE,@Spark}
21/07/04 20:48:41.969 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@9b906ff{/api,null,AVAILABLE,@Spark}
21/07/04 20:48:41.971 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@28523d21{/jobs/job/kill,null,AVAILABLE,@Spark}
21/07/04 20:48:41.973 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6e1596f0{/stages/stage/kill,null,AVAILABLE,@Spark}
21/07/04 20:48:41.975 Thread-3 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.43.54:4042
21/07/04 20:48:42.554 Thread-3 INFO Executor: Starting executor ID driver on host 192.168.43.54
21/07/04 20:48:42.609 Thread-3 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35413.
21/07/04 20:48:42.609 Thread-3 INFO NettyBlockTransferService: Server created on 192.168.43.54:35413
21/07/04 20:48:42.615 Thread-3 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/04 20:48:42.635 Thread-3 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.43.54, 35413, None)
21/07/04 20:48:42.643 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager 192.168.43.54:35413 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.43.54, 35413, None)
21/07/04 20:48:42.652 Thread-3 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.43.54, 35413, None)
21/07/04 20:48:42.653 Thread-3 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.43.54, 35413, None)
21/07/04 20:48:43.026 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3c3d879f{/metrics/json,null,AVAILABLE,@Spark}
21/07/04 20:48:44.096 Thread-3 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse').
21/07/04 20:48:44.096 Thread-3 INFO SharedState: Warehouse path is 'file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse'.
21/07/04 20:48:44.139 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6aa00ab4{/SQL,null,AVAILABLE,@Spark}
21/07/04 20:48:44.145 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2ed7fd95{/SQL/json,null,AVAILABLE,@Spark}
21/07/04 20:48:44.147 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1419e126{/SQL/execution,null,AVAILABLE,@Spark}
21/07/04 20:48:44.151 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@26fcddcb{/SQL/execution/json,null,AVAILABLE,@Spark}
21/07/04 20:48:44.250 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2c1b90b9{/static/sql,null,AVAILABLE,@Spark}
21/07/04 20:51:19.433 main WARN Utils: Your hostname, Yoga resolves to a loopback address: 127.0.1.1; using 192.168.43.54 instead (on interface wlp4s0)
21/07/04 20:51:19.436 main WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/07/04 20:51:21.115 main WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/07/04 20:51:21.475 main INFO SecurityManager: Changing view acls to: daniel
21/07/04 20:51:21.477 main INFO SecurityManager: Changing modify acls to: daniel
21/07/04 20:51:21.479 main INFO SecurityManager: Changing view acls groups to: 
21/07/04 20:51:21.484 main INFO SecurityManager: Changing modify acls groups to: 
21/07/04 20:51:21.486 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/04 20:51:22.293 Thread-3 INFO SparkContext: Running Spark version 3.1.2
21/07/04 20:51:22.415 Thread-3 INFO ResourceUtils: ==============================================================
21/07/04 20:51:22.416 Thread-3 INFO ResourceUtils: No custom resources configured for spark.driver.
21/07/04 20:51:22.417 Thread-3 INFO ResourceUtils: ==============================================================
21/07/04 20:51:22.418 Thread-3 INFO SparkContext: Submitted application: myApp
21/07/04 20:51:22.479 Thread-3 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/07/04 20:51:22.513 Thread-3 INFO ResourceProfile: Limiting resource is cpu
21/07/04 20:51:22.514 Thread-3 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/07/04 20:51:22.640 Thread-3 INFO SecurityManager: Changing view acls to: daniel
21/07/04 20:51:22.643 Thread-3 INFO SecurityManager: Changing modify acls to: daniel
21/07/04 20:51:22.644 Thread-3 INFO SecurityManager: Changing view acls groups to: 
21/07/04 20:51:22.644 Thread-3 INFO SecurityManager: Changing modify acls groups to: 
21/07/04 20:51:22.645 Thread-3 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/04 20:51:23.590 Thread-3 INFO Utils: Successfully started service 'sparkDriver' on port 41663.
21/07/04 20:51:23.681 Thread-3 INFO SparkEnv: Registering MapOutputTracker
21/07/04 20:51:23.766 Thread-3 INFO SparkEnv: Registering BlockManagerMaster
21/07/04 20:51:23.817 Thread-3 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/04 20:51:23.818 Thread-3 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/04 20:51:23.827 Thread-3 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/04 20:51:23.854 Thread-3 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cb02fd7a-531f-4c6a-a6d2-be61a41046d0
21/07/04 20:51:23.892 Thread-3 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
21/07/04 20:51:23.921 Thread-3 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/04 20:51:24.247 Thread-3 INFO log: Logging initialized @11770ms to org.sparkproject.jetty.util.log.Slf4jLog
21/07/04 20:51:24.476 Thread-3 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-Ubuntu-0ubuntu2.20.04
21/07/04 20:51:24.510 Thread-3 INFO Server: Started @12036ms
21/07/04 20:51:24.560 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
21/07/04 20:51:24.561 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
21/07/04 20:51:24.562 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
21/07/04 20:51:24.580 Thread-3 INFO AbstractConnector: Started ServerConnector@67ef4a57{HTTP/1.1, (http/1.1)}{0.0.0.0:4043}
21/07/04 20:51:24.580 Thread-3 INFO Utils: Successfully started service 'SparkUI' on port 4043.
21/07/04 20:51:24.654 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@19f5ac9{/jobs,null,AVAILABLE,@Spark}
21/07/04 20:51:24.661 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@39b69686{/jobs/json,null,AVAILABLE,@Spark}
21/07/04 20:51:24.666 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3008533{/jobs/job,null,AVAILABLE,@Spark}
21/07/04 20:51:24.684 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2914fcb9{/jobs/job/json,null,AVAILABLE,@Spark}
21/07/04 20:51:24.688 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@165a3219{/stages,null,AVAILABLE,@Spark}
21/07/04 20:51:24.690 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@53946b91{/stages/json,null,AVAILABLE,@Spark}
21/07/04 20:51:24.693 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@30b71af6{/stages/stage,null,AVAILABLE,@Spark}
21/07/04 20:51:24.711 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5f78a412{/stages/stage/json,null,AVAILABLE,@Spark}
21/07/04 20:51:24.714 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@116ab0ed{/stages/pool,null,AVAILABLE,@Spark}
21/07/04 20:51:24.724 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@25a6383e{/stages/pool/json,null,AVAILABLE,@Spark}
21/07/04 20:51:24.742 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@638d81f6{/storage,null,AVAILABLE,@Spark}
21/07/04 20:51:24.745 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6a18a5ab{/storage/json,null,AVAILABLE,@Spark}
21/07/04 20:51:24.749 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@67b2bb07{/storage/rdd,null,AVAILABLE,@Spark}
21/07/04 20:51:24.758 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6ec93598{/storage/rdd/json,null,AVAILABLE,@Spark}
21/07/04 20:51:24.765 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@cd0e2f6{/environment,null,AVAILABLE,@Spark}
21/07/04 20:51:24.773 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@75b0c0fe{/environment/json,null,AVAILABLE,@Spark}
21/07/04 20:51:24.777 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@402fe3f7{/executors,null,AVAILABLE,@Spark}
21/07/04 20:51:24.796 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@730fea4a{/executors/json,null,AVAILABLE,@Spark}
21/07/04 20:51:24.800 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1c4a96c7{/executors/threadDump,null,AVAILABLE,@Spark}
21/07/04 20:51:24.814 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4be758d0{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/07/04 20:51:24.842 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@f52e7db{/static,null,AVAILABLE,@Spark}
21/07/04 20:51:24.845 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@513d7ccc{/,null,AVAILABLE,@Spark}
21/07/04 20:51:24.849 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@57fb3fb6{/api,null,AVAILABLE,@Spark}
21/07/04 20:51:24.853 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2b3b665{/jobs/job/kill,null,AVAILABLE,@Spark}
21/07/04 20:51:24.856 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1859493b{/stages/stage/kill,null,AVAILABLE,@Spark}
21/07/04 20:51:24.861 Thread-3 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.43.54:4043
21/07/04 20:51:25.457 Thread-3 INFO Executor: Starting executor ID driver on host 192.168.43.54
21/07/04 20:51:25.551 Thread-3 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42449.
21/07/04 20:51:25.551 Thread-3 INFO NettyBlockTransferService: Server created on 192.168.43.54:42449
21/07/04 20:51:25.555 Thread-3 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/04 20:51:25.574 Thread-3 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.43.54, 42449, None)
21/07/04 20:51:25.586 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager 192.168.43.54:42449 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.43.54, 42449, None)
21/07/04 20:51:25.594 Thread-3 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.43.54, 42449, None)
21/07/04 20:51:25.596 Thread-3 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.43.54, 42449, None)
21/07/04 20:51:26.057 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2378349d{/metrics/json,null,AVAILABLE,@Spark}
21/07/04 20:51:26.831 Thread-3 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse').
21/07/04 20:51:26.832 Thread-3 INFO SharedState: Warehouse path is 'file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse'.
21/07/04 20:51:26.871 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@26ce4a0d{/SQL,null,AVAILABLE,@Spark}
21/07/04 20:51:26.873 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@41af90da{/SQL/json,null,AVAILABLE,@Spark}
21/07/04 20:51:26.876 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@48444c10{/SQL/execution,null,AVAILABLE,@Spark}
21/07/04 20:51:26.879 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5b1d3a56{/SQL/execution/json,null,AVAILABLE,@Spark}
21/07/04 20:51:26.898 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@67533f1a{/static/sql,null,AVAILABLE,@Spark}
21/07/04 20:54:36.946 main WARN Utils: Your hostname, Yoga resolves to a loopback address: 127.0.1.1; using 192.168.43.54 instead (on interface wlp4s0)
21/07/04 20:54:36.950 main WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/07/04 20:54:39.105 main WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/07/04 20:54:39.663 main INFO SecurityManager: Changing view acls to: daniel
21/07/04 20:54:39.675 main INFO SecurityManager: Changing modify acls to: daniel
21/07/04 20:54:39.678 main INFO SecurityManager: Changing view acls groups to: 
21/07/04 20:54:39.679 main INFO SecurityManager: Changing modify acls groups to: 
21/07/04 20:54:39.683 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/04 20:54:40.786 Thread-3 INFO SparkContext: Running Spark version 3.1.2
21/07/04 20:54:41.025 Thread-3 INFO ResourceUtils: ==============================================================
21/07/04 20:54:41.026 Thread-3 INFO ResourceUtils: No custom resources configured for spark.driver.
21/07/04 20:54:41.029 Thread-3 INFO ResourceUtils: ==============================================================
21/07/04 20:54:41.034 Thread-3 INFO SparkContext: Submitted application: myApp
21/07/04 20:54:41.095 Thread-3 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/07/04 20:54:41.123 Thread-3 INFO ResourceProfile: Limiting resource is cpu
21/07/04 20:54:41.124 Thread-3 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/07/04 20:54:41.335 Thread-3 INFO SecurityManager: Changing view acls to: daniel
21/07/04 20:54:41.339 Thread-3 INFO SecurityManager: Changing modify acls to: daniel
21/07/04 20:54:41.340 Thread-3 INFO SecurityManager: Changing view acls groups to: 
21/07/04 20:54:41.340 Thread-3 INFO SecurityManager: Changing modify acls groups to: 
21/07/04 20:54:41.342 Thread-3 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/04 20:54:42.583 Thread-3 INFO Utils: Successfully started service 'sparkDriver' on port 39265.
21/07/04 20:54:42.974 Thread-3 INFO SparkEnv: Registering MapOutputTracker
21/07/04 20:54:43.220 Thread-3 INFO SparkEnv: Registering BlockManagerMaster
21/07/04 20:54:43.327 Thread-3 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/04 20:54:43.331 Thread-3 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/04 20:54:43.378 Thread-3 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/04 20:54:43.494 Thread-3 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a587d9a7-3be6-4239-84be-167871ad0a2c
21/07/04 20:54:43.621 Thread-3 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
21/07/04 20:54:43.728 Thread-3 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/04 20:54:43.969 Thread-3 INFO log: Logging initialized @13185ms to org.sparkproject.jetty.util.log.Slf4jLog
21/07/04 20:54:44.152 Thread-3 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-Ubuntu-0ubuntu2.20.04
21/07/04 20:54:44.196 Thread-3 INFO Server: Started @13416ms
21/07/04 20:54:44.260 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
21/07/04 20:54:44.271 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
21/07/04 20:54:44.273 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
21/07/04 20:54:44.275 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
21/07/04 20:54:44.320 Thread-3 INFO AbstractConnector: Started ServerConnector@6de59350{HTTP/1.1, (http/1.1)}{0.0.0.0:4044}
21/07/04 20:54:44.320 Thread-3 INFO Utils: Successfully started service 'SparkUI' on port 4044.
21/07/04 20:54:44.541 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4e89e6ca{/jobs,null,AVAILABLE,@Spark}
21/07/04 20:54:44.558 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@769e9b19{/jobs/json,null,AVAILABLE,@Spark}
21/07/04 20:54:44.560 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@33362e0e{/jobs/job,null,AVAILABLE,@Spark}
21/07/04 20:54:44.592 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@12e9acbb{/jobs/job/json,null,AVAILABLE,@Spark}
21/07/04 20:54:44.595 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@249e5071{/stages,null,AVAILABLE,@Spark}
21/07/04 20:54:44.597 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@233bdc0d{/stages/json,null,AVAILABLE,@Spark}
21/07/04 20:54:44.600 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7299324e{/stages/stage,null,AVAILABLE,@Spark}
21/07/04 20:54:44.609 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7420030a{/stages/stage/json,null,AVAILABLE,@Spark}
21/07/04 20:54:44.615 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4de01112{/stages/pool,null,AVAILABLE,@Spark}
21/07/04 20:54:44.643 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3288eed1{/stages/pool/json,null,AVAILABLE,@Spark}
21/07/04 20:54:44.646 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@614ab29a{/storage,null,AVAILABLE,@Spark}
21/07/04 20:54:44.657 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@760d8529{/storage/json,null,AVAILABLE,@Spark}
21/07/04 20:54:44.659 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@142ad672{/storage/rdd,null,AVAILABLE,@Spark}
21/07/04 20:54:44.660 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3dcebd6e{/storage/rdd/json,null,AVAILABLE,@Spark}
21/07/04 20:54:44.663 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5c02acd4{/environment,null,AVAILABLE,@Spark}
21/07/04 20:54:44.681 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@313953d9{/environment/json,null,AVAILABLE,@Spark}
21/07/04 20:54:44.690 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2d195133{/executors,null,AVAILABLE,@Spark}
21/07/04 20:54:44.710 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@aef3480{/executors/json,null,AVAILABLE,@Spark}
21/07/04 20:54:44.713 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@28423771{/executors/threadDump,null,AVAILABLE,@Spark}
21/07/04 20:54:44.723 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3297773a{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/07/04 20:54:44.742 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6f3d2c0b{/static,null,AVAILABLE,@Spark}
21/07/04 20:54:44.745 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6b34d344{/,null,AVAILABLE,@Spark}
21/07/04 20:54:44.749 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7ef0a21{/api,null,AVAILABLE,@Spark}
21/07/04 20:54:44.752 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@43c55a07{/jobs/job/kill,null,AVAILABLE,@Spark}
21/07/04 20:54:44.754 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4fa4da2a{/stages/stage/kill,null,AVAILABLE,@Spark}
21/07/04 20:54:44.759 Thread-3 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.43.54:4044
21/07/04 20:54:45.370 Thread-3 INFO Executor: Starting executor ID driver on host 192.168.43.54
21/07/04 20:54:45.437 Thread-3 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45949.
21/07/04 20:54:45.437 Thread-3 INFO NettyBlockTransferService: Server created on 192.168.43.54:45949
21/07/04 20:54:45.445 Thread-3 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/04 20:54:45.465 Thread-3 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.43.54, 45949, None)
21/07/04 20:54:45.473 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager 192.168.43.54:45949 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.43.54, 45949, None)
21/07/04 20:54:45.497 Thread-3 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.43.54, 45949, None)
21/07/04 20:54:45.503 Thread-3 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.43.54, 45949, None)
21/07/04 20:54:46.374 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@45f016ff{/metrics/json,null,AVAILABLE,@Spark}
21/07/04 20:54:48.925 Thread-3 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse').
21/07/04 20:54:48.936 Thread-3 INFO SharedState: Warehouse path is 'file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse'.
21/07/04 20:54:49.056 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5860a2c8{/SQL,null,AVAILABLE,@Spark}
21/07/04 20:54:49.070 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1d5b2d01{/SQL/json,null,AVAILABLE,@Spark}
21/07/04 20:54:49.076 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6da45af0{/SQL/execution,null,AVAILABLE,@Spark}
21/07/04 20:54:49.088 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7d3d28f0{/SQL/execution/json,null,AVAILABLE,@Spark}
21/07/04 20:54:49.183 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@30bfbdff{/static/sql,null,AVAILABLE,@Spark}
21/07/04 23:37:59.466 main WARN Utils: Your hostname, Yoga resolves to a loopback address: 127.0.1.1; using 192.168.0.95 instead (on interface wlp4s0)
21/07/04 23:37:59.469 main WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/07/04 23:38:00.508 main WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/07/04 23:38:00.756 main INFO SecurityManager: Changing view acls to: daniel
21/07/04 23:38:00.757 main INFO SecurityManager: Changing modify acls to: daniel
21/07/04 23:38:00.758 main INFO SecurityManager: Changing view acls groups to: 
21/07/04 23:38:00.759 main INFO SecurityManager: Changing modify acls groups to: 
21/07/04 23:38:00.760 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/04 23:38:01.184 Thread-3 INFO SparkContext: Running Spark version 3.1.2
21/07/04 23:38:01.269 Thread-3 INFO ResourceUtils: ==============================================================
21/07/04 23:38:01.271 Thread-3 INFO ResourceUtils: No custom resources configured for spark.driver.
21/07/04 23:38:01.272 Thread-3 INFO ResourceUtils: ==============================================================
21/07/04 23:38:01.273 Thread-3 INFO SparkContext: Submitted application: myApp
21/07/04 23:38:01.321 Thread-3 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/07/04 23:38:01.354 Thread-3 INFO ResourceProfile: Limiting resource is cpu
21/07/04 23:38:01.356 Thread-3 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/07/04 23:38:01.453 Thread-3 INFO SecurityManager: Changing view acls to: daniel
21/07/04 23:38:01.454 Thread-3 INFO SecurityManager: Changing modify acls to: daniel
21/07/04 23:38:01.454 Thread-3 INFO SecurityManager: Changing view acls groups to: 
21/07/04 23:38:01.455 Thread-3 INFO SecurityManager: Changing modify acls groups to: 
21/07/04 23:38:01.455 Thread-3 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/04 23:38:01.922 Thread-3 INFO Utils: Successfully started service 'sparkDriver' on port 37347.
21/07/04 23:38:01.976 Thread-3 INFO SparkEnv: Registering MapOutputTracker
21/07/04 23:38:02.039 Thread-3 INFO SparkEnv: Registering BlockManagerMaster
21/07/04 23:38:02.090 Thread-3 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/04 23:38:02.091 Thread-3 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/04 23:38:02.101 Thread-3 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/04 23:38:02.124 Thread-3 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0f77329d-8120-4040-8c2c-490a0f994cf2
21/07/04 23:38:02.160 Thread-3 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
21/07/04 23:38:02.190 Thread-3 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/04 23:38:02.354 Thread-3 INFO log: Logging initialized @6665ms to org.sparkproject.jetty.util.log.Slf4jLog
21/07/04 23:38:02.511 Thread-3 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-Ubuntu-0ubuntu2.20.04
21/07/04 23:38:02.540 Thread-3 INFO Server: Started @6853ms
21/07/04 23:38:02.603 Thread-3 INFO AbstractConnector: Started ServerConnector@42e699a1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
21/07/04 23:38:02.604 Thread-3 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/07/04 23:38:02.644 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@c23d4c7{/jobs,null,AVAILABLE,@Spark}
21/07/04 23:38:02.650 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@f0792c2{/jobs/json,null,AVAILABLE,@Spark}
21/07/04 23:38:02.652 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3bb8b818{/jobs/job,null,AVAILABLE,@Spark}
21/07/04 23:38:02.663 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1a6f2e7{/jobs/job/json,null,AVAILABLE,@Spark}
21/07/04 23:38:02.665 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@a0136a7{/stages,null,AVAILABLE,@Spark}
21/07/04 23:38:02.668 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7e92001{/stages/json,null,AVAILABLE,@Spark}
21/07/04 23:38:02.670 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@712fb33c{/stages/stage,null,AVAILABLE,@Spark}
21/07/04 23:38:02.674 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@368fbccd{/stages/stage/json,null,AVAILABLE,@Spark}
21/07/04 23:38:02.676 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6d8f3fdd{/stages/pool,null,AVAILABLE,@Spark}
21/07/04 23:38:02.678 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@77ad845b{/stages/pool/json,null,AVAILABLE,@Spark}
21/07/04 23:38:02.680 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@297aa593{/storage,null,AVAILABLE,@Spark}
21/07/04 23:38:02.683 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@775221d6{/storage/json,null,AVAILABLE,@Spark}
21/07/04 23:38:02.685 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4e03b8a9{/storage/rdd,null,AVAILABLE,@Spark}
21/07/04 23:38:02.687 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@73def57c{/storage/rdd/json,null,AVAILABLE,@Spark}
21/07/04 23:38:02.690 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5c340a2d{/environment,null,AVAILABLE,@Spark}
21/07/04 23:38:02.692 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6c41ded1{/environment/json,null,AVAILABLE,@Spark}
21/07/04 23:38:02.694 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1613297a{/executors,null,AVAILABLE,@Spark}
21/07/04 23:38:02.696 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1906bb38{/executors/json,null,AVAILABLE,@Spark}
21/07/04 23:38:02.698 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2d7b0fb9{/executors/threadDump,null,AVAILABLE,@Spark}
21/07/04 23:38:02.713 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6014b771{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/07/04 23:38:02.747 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@525fd355{/static,null,AVAILABLE,@Spark}
21/07/04 23:38:02.749 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1cb64d2e{/,null,AVAILABLE,@Spark}
21/07/04 23:38:02.755 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3000f0ba{/api,null,AVAILABLE,@Spark}
21/07/04 23:38:02.757 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@77065ed9{/jobs/job/kill,null,AVAILABLE,@Spark}
21/07/04 23:38:02.759 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7c6a2239{/stages/stage/kill,null,AVAILABLE,@Spark}
21/07/04 23:38:02.764 Thread-3 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.95:4040
21/07/04 23:38:03.219 Thread-3 INFO Executor: Starting executor ID driver on host 192.168.0.95
21/07/04 23:38:03.275 Thread-3 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45285.
21/07/04 23:38:03.276 Thread-3 INFO NettyBlockTransferService: Server created on 192.168.0.95:45285
21/07/04 23:38:03.280 Thread-3 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/04 23:38:03.298 Thread-3 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.95, 45285, None)
21/07/04 23:38:03.309 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.95:45285 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.95, 45285, None)
21/07/04 23:38:03.316 Thread-3 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.95, 45285, None)
21/07/04 23:38:03.318 Thread-3 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.95, 45285, None)
21/07/04 23:38:03.631 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7fc028f{/metrics/json,null,AVAILABLE,@Spark}
21/07/04 23:38:04.246 Thread-3 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse').
21/07/04 23:38:04.247 Thread-3 INFO SharedState: Warehouse path is 'file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse'.
21/07/04 23:38:04.279 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1033e798{/SQL,null,AVAILABLE,@Spark}
21/07/04 23:38:04.281 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4f748b3a{/SQL/json,null,AVAILABLE,@Spark}
21/07/04 23:38:04.284 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@463dd85c{/SQL/execution,null,AVAILABLE,@Spark}
21/07/04 23:38:04.286 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@505262bb{/SQL/execution/json,null,AVAILABLE,@Spark}
21/07/04 23:38:04.309 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@57d67495{/static/sql,null,AVAILABLE,@Spark}
21/07/04 23:47:06.590 main WARN Utils: Your hostname, Yoga resolves to a loopback address: 127.0.1.1; using 192.168.0.95 instead (on interface wlp4s0)
21/07/04 23:47:06.600 main WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/07/04 23:47:08.780 main WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/07/04 23:47:08.980 main INFO SecurityManager: Changing view acls to: daniel
21/07/04 23:47:08.983 main INFO SecurityManager: Changing modify acls to: daniel
21/07/04 23:47:08.984 main INFO SecurityManager: Changing view acls groups to: 
21/07/04 23:47:08.985 main INFO SecurityManager: Changing modify acls groups to: 
21/07/04 23:47:08.986 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/04 23:47:09.893 Thread-3 INFO SparkContext: Running Spark version 3.1.2
21/07/04 23:47:10.346 Thread-3 INFO ResourceUtils: ==============================================================
21/07/04 23:47:10.348 Thread-3 INFO ResourceUtils: No custom resources configured for spark.driver.
21/07/04 23:47:10.360 Thread-3 INFO ResourceUtils: ==============================================================
21/07/04 23:47:10.361 Thread-3 INFO SparkContext: Submitted application: myApp
21/07/04 23:47:10.823 Thread-3 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/07/04 23:47:10.924 Thread-3 INFO ResourceProfile: Limiting resource is cpu
21/07/04 23:47:10.928 Thread-3 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/07/04 23:47:11.112 Thread-3 INFO SecurityManager: Changing view acls to: daniel
21/07/04 23:47:11.113 Thread-3 INFO SecurityManager: Changing modify acls to: daniel
21/07/04 23:47:11.113 Thread-3 INFO SecurityManager: Changing view acls groups to: 
21/07/04 23:47:11.114 Thread-3 INFO SecurityManager: Changing modify acls groups to: 
21/07/04 23:47:11.114 Thread-3 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/04 23:47:12.455 Thread-3 INFO Utils: Successfully started service 'sparkDriver' on port 36329.
21/07/04 23:47:12.525 Thread-3 INFO SparkEnv: Registering MapOutputTracker
21/07/04 23:47:12.639 Thread-3 INFO SparkEnv: Registering BlockManagerMaster
21/07/04 23:47:12.752 Thread-3 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/04 23:47:12.754 Thread-3 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/04 23:47:12.765 Thread-3 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/04 23:47:12.797 Thread-3 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-57002dd0-c189-4d35-be43-41fab8b6829d
21/07/04 23:47:12.843 Thread-3 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
21/07/04 23:47:12.908 Thread-3 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/04 23:47:13.098 Thread-3 INFO log: Logging initialized @13000ms to org.sparkproject.jetty.util.log.Slf4jLog
21/07/04 23:47:13.252 Thread-3 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-Ubuntu-0ubuntu2.20.04
21/07/04 23:47:13.310 Thread-3 INFO Server: Started @13214ms
21/07/04 23:47:13.366 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
21/07/04 23:47:13.392 Thread-3 INFO AbstractConnector: Started ServerConnector@a909944{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}
21/07/04 23:47:13.392 Thread-3 INFO Utils: Successfully started service 'SparkUI' on port 4041.
21/07/04 23:47:13.526 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7a08455f{/jobs,null,AVAILABLE,@Spark}
21/07/04 23:47:13.531 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@45732621{/jobs/json,null,AVAILABLE,@Spark}
21/07/04 23:47:13.533 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5cfc5eb3{/jobs/job,null,AVAILABLE,@Spark}
21/07/04 23:47:13.549 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@696ec245{/jobs/job/json,null,AVAILABLE,@Spark}
21/07/04 23:47:13.560 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@69d4b9d5{/stages,null,AVAILABLE,@Spark}
21/07/04 23:47:13.563 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@12dc1e60{/stages/json,null,AVAILABLE,@Spark}
21/07/04 23:47:13.565 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@81f8c00{/stages/stage,null,AVAILABLE,@Spark}
21/07/04 23:47:13.570 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6cc354c3{/stages/stage/json,null,AVAILABLE,@Spark}
21/07/04 23:47:13.573 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@477cb773{/stages/pool,null,AVAILABLE,@Spark}
21/07/04 23:47:13.576 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4e3d2139{/stages/pool/json,null,AVAILABLE,@Spark}
21/07/04 23:47:13.579 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7f96f5ba{/storage,null,AVAILABLE,@Spark}
21/07/04 23:47:13.582 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@109e655d{/storage/json,null,AVAILABLE,@Spark}
21/07/04 23:47:13.585 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1d69f4fc{/storage/rdd,null,AVAILABLE,@Spark}
21/07/04 23:47:13.589 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@241ae64a{/storage/rdd/json,null,AVAILABLE,@Spark}
21/07/04 23:47:13.592 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@749850c1{/environment,null,AVAILABLE,@Spark}
21/07/04 23:47:13.594 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2e9cf22a{/environment/json,null,AVAILABLE,@Spark}
21/07/04 23:47:13.599 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1a450152{/executors,null,AVAILABLE,@Spark}
21/07/04 23:47:13.603 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@351b49f0{/executors/json,null,AVAILABLE,@Spark}
21/07/04 23:47:13.607 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@41df12d0{/executors/threadDump,null,AVAILABLE,@Spark}
21/07/04 23:47:13.611 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2d777e09{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/07/04 23:47:13.631 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7153e2ec{/static,null,AVAILABLE,@Spark}
21/07/04 23:47:13.633 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@62a10aba{/,null,AVAILABLE,@Spark}
21/07/04 23:47:13.638 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2f4818c2{/api,null,AVAILABLE,@Spark}
21/07/04 23:47:13.642 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7200ea71{/jobs/job/kill,null,AVAILABLE,@Spark}
21/07/04 23:47:13.645 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6004d65f{/stages/stage/kill,null,AVAILABLE,@Spark}
21/07/04 23:47:13.660 Thread-3 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.95:4041
21/07/04 23:47:14.711 Thread-3 INFO Executor: Starting executor ID driver on host 192.168.0.95
21/07/04 23:47:14.841 Thread-3 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34403.
21/07/04 23:47:14.841 Thread-3 INFO NettyBlockTransferService: Server created on 192.168.0.95:34403
21/07/04 23:47:14.844 Thread-3 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/04 23:47:14.877 Thread-3 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.95, 34403, None)
21/07/04 23:47:14.898 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.95:34403 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.95, 34403, None)
21/07/04 23:47:14.904 Thread-3 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.95, 34403, None)
21/07/04 23:47:14.907 Thread-3 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.95, 34403, None)
21/07/04 23:47:15.328 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6d45eeed{/metrics/json,null,AVAILABLE,@Spark}
21/07/04 23:47:16.116 Thread-3 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse').
21/07/04 23:47:16.117 Thread-3 INFO SharedState: Warehouse path is 'file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse'.
21/07/04 23:47:16.156 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2e19318c{/SQL,null,AVAILABLE,@Spark}
21/07/04 23:47:16.159 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6d036a90{/SQL/json,null,AVAILABLE,@Spark}
21/07/04 23:47:16.162 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@59a513d8{/SQL/execution,null,AVAILABLE,@Spark}
21/07/04 23:47:16.166 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5e16f32e{/SQL/execution/json,null,AVAILABLE,@Spark}
21/07/04 23:47:16.194 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7fdfc337{/static/sql,null,AVAILABLE,@Spark}
21/07/04 23:50:36.888 stream execution thread for [id = 03301685-417e-4ded-99ba-b5ae3bc8d733, runId = f76b10a4-76e1-48f8-baa9-5bc54709a87d] ERROR MicroBatchExecution: Query [id = 03301685-417e-4ded-99ba-b5ae3bc8d733, runId = f76b10a4-76e1-48f8-baa9-5bc54709a87d] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.kafka010.KafkaConfigUpdater
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	... 22 more
21/07/05 00:09:29.323 stream execution thread for [id = 3dd87f18-ab71-406d-8722-0bdd5ff6ca49, runId = 7fb9bd26-0cce-4004-8907-356e31291f9f] ERROR MicroBatchExecution: Query [id = 3dd87f18-ab71-406d-8722-0bdd5ff6ca49, runId = 7fb9bd26-0cce-4004-8907-356e31291f9f] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/05 00:10:24.592 main WARN Utils: Your hostname, Yoga resolves to a loopback address: 127.0.1.1; using 192.168.0.95 instead (on interface wlp4s0)
21/07/05 00:10:24.595 main WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/07/05 00:10:25.817 main WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/07/05 00:10:26.073 main INFO SecurityManager: Changing view acls to: daniel
21/07/05 00:10:26.075 main INFO SecurityManager: Changing modify acls to: daniel
21/07/05 00:10:26.080 main INFO SecurityManager: Changing view acls groups to: 
21/07/05 00:10:26.082 main INFO SecurityManager: Changing modify acls groups to: 
21/07/05 00:10:26.084 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/05 00:10:26.550 Thread-3 INFO SparkContext: Running Spark version 3.1.2
21/07/05 00:10:26.662 Thread-3 INFO ResourceUtils: ==============================================================
21/07/05 00:10:26.663 Thread-3 INFO ResourceUtils: No custom resources configured for spark.driver.
21/07/05 00:10:26.664 Thread-3 INFO ResourceUtils: ==============================================================
21/07/05 00:10:26.665 Thread-3 INFO SparkContext: Submitted application: myApp
21/07/05 00:10:26.727 Thread-3 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/07/05 00:10:26.775 Thread-3 INFO ResourceProfile: Limiting resource is cpu
21/07/05 00:10:26.777 Thread-3 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/07/05 00:10:26.885 Thread-3 INFO SecurityManager: Changing view acls to: daniel
21/07/05 00:10:26.887 Thread-3 INFO SecurityManager: Changing modify acls to: daniel
21/07/05 00:10:26.888 Thread-3 INFO SecurityManager: Changing view acls groups to: 
21/07/05 00:10:26.889 Thread-3 INFO SecurityManager: Changing modify acls groups to: 
21/07/05 00:10:26.889 Thread-3 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/05 00:10:27.600 Thread-3 INFO Utils: Successfully started service 'sparkDriver' on port 41229.
21/07/05 00:10:27.697 Thread-3 INFO SparkEnv: Registering MapOutputTracker
21/07/05 00:10:27.898 Thread-3 INFO SparkEnv: Registering BlockManagerMaster
21/07/05 00:10:28.012 Thread-3 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/05 00:10:28.014 Thread-3 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/05 00:10:28.027 Thread-3 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/05 00:10:28.126 Thread-3 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4060a7f0-f340-4495-858b-50080bb02cea
21/07/05 00:10:28.223 Thread-3 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
21/07/05 00:10:28.313 Thread-3 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/05 00:10:28.622 Thread-3 INFO log: Logging initialized @9079ms to org.sparkproject.jetty.util.log.Slf4jLog
21/07/05 00:10:28.738 Thread-3 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-Ubuntu-0ubuntu2.20.04
21/07/05 00:10:28.816 Thread-3 INFO Server: Started @9275ms
21/07/05 00:10:28.866 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
21/07/05 00:10:28.867 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
21/07/05 00:10:28.891 Thread-3 INFO AbstractConnector: Started ServerConnector@36211e10{HTTP/1.1, (http/1.1)}{0.0.0.0:4042}
21/07/05 00:10:28.891 Thread-3 INFO Utils: Successfully started service 'SparkUI' on port 4042.
21/07/05 00:10:28.964 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4322d043{/jobs,null,AVAILABLE,@Spark}
21/07/05 00:10:28.968 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2a4038be{/jobs/json,null,AVAILABLE,@Spark}
21/07/05 00:10:28.970 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@18a526c5{/jobs/job,null,AVAILABLE,@Spark}
21/07/05 00:10:28.977 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@34369fd3{/jobs/job/json,null,AVAILABLE,@Spark}
21/07/05 00:10:28.985 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@e2f9c59{/stages,null,AVAILABLE,@Spark}
21/07/05 00:10:28.987 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@e3248df{/stages/json,null,AVAILABLE,@Spark}
21/07/05 00:10:28.991 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@22170d74{/stages/stage,null,AVAILABLE,@Spark}
21/07/05 00:10:28.999 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@30a940d8{/stages/stage/json,null,AVAILABLE,@Spark}
21/07/05 00:10:29.006 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1c1de7af{/stages/pool,null,AVAILABLE,@Spark}
21/07/05 00:10:29.009 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1b60ad5b{/stages/pool/json,null,AVAILABLE,@Spark}
21/07/05 00:10:29.014 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2a5522e3{/storage,null,AVAILABLE,@Spark}
21/07/05 00:10:29.019 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@79c8266f{/storage/json,null,AVAILABLE,@Spark}
21/07/05 00:10:29.025 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@17556c60{/storage/rdd,null,AVAILABLE,@Spark}
21/07/05 00:10:29.037 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2a622d50{/storage/rdd/json,null,AVAILABLE,@Spark}
21/07/05 00:10:29.042 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@22f90aab{/environment,null,AVAILABLE,@Spark}
21/07/05 00:10:29.049 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7247e1c{/environment/json,null,AVAILABLE,@Spark}
21/07/05 00:10:29.053 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@698e639a{/executors,null,AVAILABLE,@Spark}
21/07/05 00:10:29.055 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4f080971{/executors/json,null,AVAILABLE,@Spark}
21/07/05 00:10:29.057 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@12674de3{/executors/threadDump,null,AVAILABLE,@Spark}
21/07/05 00:10:29.061 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@340e3815{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/07/05 00:10:29.091 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6f5cbf4f{/static,null,AVAILABLE,@Spark}
21/07/05 00:10:29.093 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@27c26d4e{/,null,AVAILABLE,@Spark}
21/07/05 00:10:29.102 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@348634e3{/api,null,AVAILABLE,@Spark}
21/07/05 00:10:29.107 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6ffa7bbc{/jobs/job/kill,null,AVAILABLE,@Spark}
21/07/05 00:10:29.115 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@53527be9{/stages/stage/kill,null,AVAILABLE,@Spark}
21/07/05 00:10:29.122 Thread-3 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.95:4042
21/07/05 00:10:29.670 Thread-3 INFO Executor: Starting executor ID driver on host 192.168.0.95
21/07/05 00:10:29.729 Thread-3 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39571.
21/07/05 00:10:29.730 Thread-3 INFO NettyBlockTransferService: Server created on 192.168.0.95:39571
21/07/05 00:10:29.733 Thread-3 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/05 00:10:29.753 Thread-3 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.95, 39571, None)
21/07/05 00:10:29.762 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.95:39571 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.95, 39571, None)
21/07/05 00:10:29.773 Thread-3 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.95, 39571, None)
21/07/05 00:10:29.776 Thread-3 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.95, 39571, None)
21/07/05 00:10:30.192 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@dfd2f7e{/metrics/json,null,AVAILABLE,@Spark}
21/07/05 00:10:31.005 Thread-3 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse').
21/07/05 00:10:31.006 Thread-3 INFO SharedState: Warehouse path is 'file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse'.
21/07/05 00:10:31.042 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2530b87a{/SQL,null,AVAILABLE,@Spark}
21/07/05 00:10:31.044 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4b3bb15e{/SQL/json,null,AVAILABLE,@Spark}
21/07/05 00:10:31.047 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1e56acb7{/SQL/execution,null,AVAILABLE,@Spark}
21/07/05 00:10:31.051 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6da2076f{/SQL/execution/json,null,AVAILABLE,@Spark}
21/07/05 00:10:31.077 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@30d13b0a{/static/sql,null,AVAILABLE,@Spark}
21/07/05 00:10:46.973 stream execution thread for [id = 034dd6b8-6f68-4e8c-81f4-adb1c777af97, runId = 2bdab545-d028-47fe-b342-b7b85917bc9e] ERROR MicroBatchExecution: Query [id = 034dd6b8-6f68-4e8c-81f4-adb1c777af97, runId = 2bdab545-d028-47fe-b342-b7b85917bc9e] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.kafka010.KafkaConfigUpdater
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	... 22 more
21/07/05 00:17:08.226 stream execution thread for [id = e680c8c5-8fe5-413e-a5f5-62ca3392e041, runId = 71ada8f0-57da-4cc6-bde1-88389dbee0af] ERROR MicroBatchExecution: Query [id = e680c8c5-8fe5-413e-a5f5-62ca3392e041, runId = 71ada8f0-57da-4cc6-bde1-88389dbee0af] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/05 00:18:46.450 stream execution thread for [id = d2fc0ce1-da82-4fb2-9bc3-aecf1c78428f, runId = 69f0786d-0c14-44d9-8a20-efde57338994] ERROR MicroBatchExecution: Query [id = d2fc0ce1-da82-4fb2-9bc3-aecf1c78428f, runId = 69f0786d-0c14-44d9-8a20-efde57338994] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/05 08:16:42.457 main WARN Utils: Your hostname, Yoga resolves to a loopback address: 127.0.1.1; using 192.168.0.95 instead (on interface wlp4s0)
21/07/05 08:16:42.463 main WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/07/05 08:16:44.639 main WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/07/05 08:16:45.058 main INFO SecurityManager: Changing view acls to: daniel
21/07/05 08:16:45.060 main INFO SecurityManager: Changing modify acls to: daniel
21/07/05 08:16:45.061 main INFO SecurityManager: Changing view acls groups to: 
21/07/05 08:16:45.062 main INFO SecurityManager: Changing modify acls groups to: 
21/07/05 08:16:45.063 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/05 08:16:45.654 Thread-3 INFO SparkContext: Running Spark version 3.1.2
21/07/05 08:16:45.861 Thread-3 INFO ResourceUtils: ==============================================================
21/07/05 08:16:45.863 Thread-3 INFO ResourceUtils: No custom resources configured for spark.driver.
21/07/05 08:16:45.864 Thread-3 INFO ResourceUtils: ==============================================================
21/07/05 08:16:45.865 Thread-3 INFO SparkContext: Submitted application: myApp
21/07/05 08:16:46.038 Thread-3 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/07/05 08:16:46.095 Thread-3 INFO ResourceProfile: Limiting resource is cpu
21/07/05 08:16:46.096 Thread-3 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/07/05 08:16:46.345 Thread-3 INFO SecurityManager: Changing view acls to: daniel
21/07/05 08:16:46.366 Thread-3 INFO SecurityManager: Changing modify acls to: daniel
21/07/05 08:16:46.368 Thread-3 INFO SecurityManager: Changing view acls groups to: 
21/07/05 08:16:46.370 Thread-3 INFO SecurityManager: Changing modify acls groups to: 
21/07/05 08:16:46.371 Thread-3 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/05 08:16:47.745 Thread-3 INFO Utils: Successfully started service 'sparkDriver' on port 33955.
21/07/05 08:16:47.863 Thread-3 INFO SparkEnv: Registering MapOutputTracker
21/07/05 08:16:47.998 Thread-3 INFO SparkEnv: Registering BlockManagerMaster
21/07/05 08:16:48.093 Thread-3 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/05 08:16:48.094 Thread-3 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/05 08:16:48.118 Thread-3 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/05 08:16:48.158 Thread-3 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-db4bc11f-5130-47c6-a4e5-d9689e108178
21/07/05 08:16:48.227 Thread-3 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
21/07/05 08:16:48.327 Thread-3 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/05 08:16:48.691 Thread-3 INFO log: Logging initialized @9152ms to org.sparkproject.jetty.util.log.Slf4jLog
21/07/05 08:16:48.896 Thread-3 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-Ubuntu-0ubuntu2.20.04
21/07/05 08:16:48.980 Thread-3 INFO Server: Started @9444ms
21/07/05 08:16:49.186 Thread-3 INFO AbstractConnector: Started ServerConnector@795301e2{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
21/07/05 08:16:49.186 Thread-3 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/07/05 08:16:49.344 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6749c15d{/jobs,null,AVAILABLE,@Spark}
21/07/05 08:16:49.350 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@24b836be{/jobs/json,null,AVAILABLE,@Spark}
21/07/05 08:16:49.352 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@18d14e99{/jobs/job,null,AVAILABLE,@Spark}
21/07/05 08:16:49.367 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2067af7c{/jobs/job/json,null,AVAILABLE,@Spark}
21/07/05 08:16:49.377 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@286a601d{/stages,null,AVAILABLE,@Spark}
21/07/05 08:16:49.385 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@32859cad{/stages/json,null,AVAILABLE,@Spark}
21/07/05 08:16:49.431 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@606bce99{/stages/stage,null,AVAILABLE,@Spark}
21/07/05 08:16:49.461 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@69fecc22{/stages/stage/json,null,AVAILABLE,@Spark}
21/07/05 08:16:49.468 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5f18fdc2{/stages/pool,null,AVAILABLE,@Spark}
21/07/05 08:16:49.475 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@50a7535d{/stages/pool/json,null,AVAILABLE,@Spark}
21/07/05 08:16:49.478 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5cb2974{/storage,null,AVAILABLE,@Spark}
21/07/05 08:16:49.496 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@18066d87{/storage/json,null,AVAILABLE,@Spark}
21/07/05 08:16:49.509 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@704401df{/storage/rdd,null,AVAILABLE,@Spark}
21/07/05 08:16:49.530 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2680e4ab{/storage/rdd/json,null,AVAILABLE,@Spark}
21/07/05 08:16:49.535 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1a03ba2a{/environment,null,AVAILABLE,@Spark}
21/07/05 08:16:49.553 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@389366cc{/environment/json,null,AVAILABLE,@Spark}
21/07/05 08:16:49.566 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3d5fadf8{/executors,null,AVAILABLE,@Spark}
21/07/05 08:16:49.590 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3c0b5eb5{/executors/json,null,AVAILABLE,@Spark}
21/07/05 08:16:49.601 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@120c4d4d{/executors/threadDump,null,AVAILABLE,@Spark}
21/07/05 08:16:49.632 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@49dbc0f{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/07/05 08:16:49.703 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@46116301{/static,null,AVAILABLE,@Spark}
21/07/05 08:16:49.709 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@42b1b159{/,null,AVAILABLE,@Spark}
21/07/05 08:16:49.717 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1be0b051{/api,null,AVAILABLE,@Spark}
21/07/05 08:16:49.720 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6afe06f2{/jobs/job/kill,null,AVAILABLE,@Spark}
21/07/05 08:16:49.724 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@53e6305e{/stages/stage/kill,null,AVAILABLE,@Spark}
21/07/05 08:16:49.730 Thread-3 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.95:4040
21/07/05 08:16:50.717 Thread-3 INFO Executor: Starting executor ID driver on host 192.168.0.95
21/07/05 08:16:51.062 Thread-3 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46135.
21/07/05 08:16:51.063 Thread-3 INFO NettyBlockTransferService: Server created on 192.168.0.95:46135
21/07/05 08:16:51.071 Thread-3 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/05 08:16:51.109 Thread-3 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.95, 46135, None)
21/07/05 08:16:51.136 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.95:46135 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.95, 46135, None)
21/07/05 08:16:51.157 Thread-3 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.95, 46135, None)
21/07/05 08:16:51.160 Thread-3 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.95, 46135, None)
21/07/05 08:16:52.479 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3fa29d2d{/metrics/json,null,AVAILABLE,@Spark}
21/07/05 08:16:54.790 Thread-3 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse').
21/07/05 08:16:54.824 Thread-3 INFO SharedState: Warehouse path is 'file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse'.
21/07/05 08:16:54.948 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@12f7a366{/SQL,null,AVAILABLE,@Spark}
21/07/05 08:16:54.965 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2767f697{/SQL/json,null,AVAILABLE,@Spark}
21/07/05 08:16:54.975 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@960ce56{/SQL/execution,null,AVAILABLE,@Spark}
21/07/05 08:16:54.977 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5ca2d7c4{/SQL/execution/json,null,AVAILABLE,@Spark}
21/07/05 08:16:55.088 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1f2bd145{/static/sql,null,AVAILABLE,@Spark}
21/07/05 08:19:09.658 stream execution thread for [id = 29c219f7-8eb2-4ba5-ae69-ad9cf76e5a03, runId = 50b3cb77-1e1b-437d-a478-aa2e7182c71c] ERROR MicroBatchExecution: Query [id = 29c219f7-8eb2-4ba5-ae69-ad9cf76e5a03, runId = 50b3cb77-1e1b-437d-a478-aa2e7182c71c] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.kafka010.KafkaConfigUpdater
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	... 33 more
21/07/05 08:32:21.063 stream execution thread for [id = 4c0b163f-4232-44c2-a6fe-6d8f947dca31, runId = 34876ae1-8c96-44e2-9d12-7a1a4c78a69c] ERROR MicroBatchExecution: Query [id = 4c0b163f-4232-44c2-a6fe-6d8f947dca31, runId = 34876ae1-8c96-44e2-9d12-7a1a4c78a69c] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/05 08:32:54.756 stream execution thread for [id = 845fd74d-3cad-47a7-b96c-7f00ab40ceba, runId = a211206b-f389-411d-a668-6ae87b265f38] ERROR MicroBatchExecution: Query [id = 845fd74d-3cad-47a7-b96c-7f00ab40ceba, runId = a211206b-f389-411d-a668-6ae87b265f38] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/05 08:38:41.111 stream execution thread for [id = 301abfc9-5876-41de-b727-191001d0c29d, runId = 1bb311a3-d761-40bc-b621-b2d4c0adae59] ERROR MicroBatchExecution: Query [id = 301abfc9-5876-41de-b727-191001d0c29d, runId = 1bb311a3-d761-40bc-b621-b2d4c0adae59] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/05 08:38:50.209 stream execution thread for [id = 8e74984b-9cac-447f-9357-6117a4233e91, runId = dba47dfb-44cb-4a3d-adb9-0f887e25dc72] ERROR MicroBatchExecution: Query [id = 8e74984b-9cac-447f-9357-6117a4233e91, runId = dba47dfb-44cb-4a3d-adb9-0f887e25dc72] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/05 08:41:10.119 main WARN Utils: Your hostname, Yoga resolves to a loopback address: 127.0.1.1; using 192.168.0.95 instead (on interface wlp4s0)
21/07/05 08:41:10.123 main WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/07/05 08:41:11.346 main WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/07/05 08:41:11.773 main INFO SecurityManager: Changing view acls to: daniel
21/07/05 08:41:11.775 main INFO SecurityManager: Changing modify acls to: daniel
21/07/05 08:41:11.777 main INFO SecurityManager: Changing view acls groups to: 
21/07/05 08:41:11.779 main INFO SecurityManager: Changing modify acls groups to: 
21/07/05 08:41:11.782 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/05 08:41:12.680 Thread-3 INFO SparkContext: Running Spark version 3.1.2
21/07/05 08:41:12.859 Thread-3 INFO ResourceUtils: ==============================================================
21/07/05 08:41:12.861 Thread-3 INFO ResourceUtils: No custom resources configured for spark.driver.
21/07/05 08:41:12.869 Thread-3 INFO ResourceUtils: ==============================================================
21/07/05 08:41:12.871 Thread-3 INFO SparkContext: Submitted application: myApp
21/07/05 08:41:12.971 Thread-3 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/07/05 08:41:13.040 Thread-3 INFO ResourceProfile: Limiting resource is cpu
21/07/05 08:41:13.042 Thread-3 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/07/05 08:41:13.178 Thread-3 INFO SecurityManager: Changing view acls to: daniel
21/07/05 08:41:13.179 Thread-3 INFO SecurityManager: Changing modify acls to: daniel
21/07/05 08:41:13.179 Thread-3 INFO SecurityManager: Changing view acls groups to: 
21/07/05 08:41:13.180 Thread-3 INFO SecurityManager: Changing modify acls groups to: 
21/07/05 08:41:13.180 Thread-3 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/05 08:41:13.750 Thread-3 INFO Utils: Successfully started service 'sparkDriver' on port 33207.
21/07/05 08:41:13.805 Thread-3 INFO SparkEnv: Registering MapOutputTracker
21/07/05 08:41:13.866 Thread-3 INFO SparkEnv: Registering BlockManagerMaster
21/07/05 08:41:13.907 Thread-3 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/05 08:41:13.908 Thread-3 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/05 08:41:13.915 Thread-3 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/05 08:41:13.935 Thread-3 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f9a79df7-2549-499e-826d-1a669c3d57f9
21/07/05 08:41:13.969 Thread-3 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
21/07/05 08:41:13.998 Thread-3 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/05 08:41:14.150 Thread-3 INFO log: Logging initialized @8834ms to org.sparkproject.jetty.util.log.Slf4jLog
21/07/05 08:41:14.246 Thread-3 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-Ubuntu-0ubuntu2.20.04
21/07/05 08:41:14.274 Thread-3 INFO Server: Started @8961ms
21/07/05 08:41:14.373 Thread-3 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
21/07/05 08:41:14.397 Thread-3 INFO AbstractConnector: Started ServerConnector@23aa965a{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}
21/07/05 08:41:14.400 Thread-3 INFO Utils: Successfully started service 'SparkUI' on port 4041.
21/07/05 08:41:14.479 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@684eacd8{/jobs,null,AVAILABLE,@Spark}
21/07/05 08:41:14.484 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1966cc84{/jobs/json,null,AVAILABLE,@Spark}
21/07/05 08:41:14.485 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@46576290{/jobs/job,null,AVAILABLE,@Spark}
21/07/05 08:41:14.491 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7ce7e9da{/jobs/job/json,null,AVAILABLE,@Spark}
21/07/05 08:41:14.493 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5e863eb6{/stages,null,AVAILABLE,@Spark}
21/07/05 08:41:14.494 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@714c8547{/stages/json,null,AVAILABLE,@Spark}
21/07/05 08:41:14.496 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3c737a37{/stages/stage,null,AVAILABLE,@Spark}
21/07/05 08:41:14.500 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@280caee0{/stages/stage/json,null,AVAILABLE,@Spark}
21/07/05 08:41:14.502 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2486e04a{/stages/pool,null,AVAILABLE,@Spark}
21/07/05 08:41:14.503 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4e9de182{/stages/pool/json,null,AVAILABLE,@Spark}
21/07/05 08:41:14.505 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@69e57f9d{/storage,null,AVAILABLE,@Spark}
21/07/05 08:41:14.507 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@12016671{/storage/json,null,AVAILABLE,@Spark}
21/07/05 08:41:14.509 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@38e1aa49{/storage/rdd,null,AVAILABLE,@Spark}
21/07/05 08:41:14.510 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1a43f3f3{/storage/rdd/json,null,AVAILABLE,@Spark}
21/07/05 08:41:14.512 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@40c5ebe{/environment,null,AVAILABLE,@Spark}
21/07/05 08:41:14.515 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4307565a{/environment/json,null,AVAILABLE,@Spark}
21/07/05 08:41:14.517 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@44ba67e3{/executors,null,AVAILABLE,@Spark}
21/07/05 08:41:14.521 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@cd175b3{/executors/json,null,AVAILABLE,@Spark}
21/07/05 08:41:14.523 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@10c80275{/executors/threadDump,null,AVAILABLE,@Spark}
21/07/05 08:41:14.532 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@8777327{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/07/05 08:41:14.549 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@8b66a1b{/static,null,AVAILABLE,@Spark}
21/07/05 08:41:14.551 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@227fd795{/,null,AVAILABLE,@Spark}
21/07/05 08:41:14.554 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@32cf5fed{/api,null,AVAILABLE,@Spark}
21/07/05 08:41:14.557 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@f87b6ab{/jobs/job/kill,null,AVAILABLE,@Spark}
21/07/05 08:41:14.559 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@67d28ba6{/stages/stage/kill,null,AVAILABLE,@Spark}
21/07/05 08:41:14.561 Thread-3 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.95:4041
21/07/05 08:41:14.951 Thread-3 INFO Executor: Starting executor ID driver on host 192.168.0.95
21/07/05 08:41:15.009 Thread-3 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32981.
21/07/05 08:41:15.009 Thread-3 INFO NettyBlockTransferService: Server created on 192.168.0.95:32981
21/07/05 08:41:15.011 Thread-3 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/05 08:41:15.025 Thread-3 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.95, 32981, None)
21/07/05 08:41:15.030 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.95:32981 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.95, 32981, None)
21/07/05 08:41:15.036 Thread-3 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.95, 32981, None)
21/07/05 08:41:15.038 Thread-3 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.95, 32981, None)
21/07/05 08:41:15.532 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1be74098{/metrics/json,null,AVAILABLE,@Spark}
21/07/05 08:41:16.097 Thread-3 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse').
21/07/05 08:41:16.098 Thread-3 INFO SharedState: Warehouse path is 'file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse'.
21/07/05 08:41:16.126 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@207459c1{/SQL,null,AVAILABLE,@Spark}
21/07/05 08:41:16.128 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5cc49ea8{/SQL/json,null,AVAILABLE,@Spark}
21/07/05 08:41:16.131 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3a136205{/SQL/execution,null,AVAILABLE,@Spark}
21/07/05 08:41:16.133 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3ab7d128{/SQL/execution/json,null,AVAILABLE,@Spark}
21/07/05 08:41:16.152 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@44ccb5fb{/static/sql,null,AVAILABLE,@Spark}
21/07/05 08:42:49.363 stream execution thread for [id = be86ce2c-da0c-41f5-a669-845970527485, runId = b2816a85-184e-41a5-9cac-bf6a6e4fbebd] ERROR MicroBatchExecution: Query [id = be86ce2c-da0c-41f5-a669-845970527485, runId = b2816a85-184e-41a5-9cac-bf6a6e4fbebd] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.kafka010.KafkaConfigUpdater
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	... 33 more
21/07/21 19:13:15.736 main WARN Utils: Your hostname, Yoga resolves to a loopback address: 127.0.1.1; using 192.168.178.70 instead (on interface wlp4s0)
21/07/21 19:13:15.739 main WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/07/21 19:13:16.863 main WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/07/21 19:13:17.136 main INFO SecurityManager: Changing view acls to: daniel
21/07/21 19:13:17.138 main INFO SecurityManager: Changing modify acls to: daniel
21/07/21 19:13:17.139 main INFO SecurityManager: Changing view acls groups to: 
21/07/21 19:13:17.141 main INFO SecurityManager: Changing modify acls groups to: 
21/07/21 19:13:17.142 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/21 19:13:17.718 Thread-3 INFO SparkContext: Running Spark version 3.1.2
21/07/21 19:13:17.830 Thread-3 INFO ResourceUtils: ==============================================================
21/07/21 19:13:17.831 Thread-3 INFO ResourceUtils: No custom resources configured for spark.driver.
21/07/21 19:13:17.832 Thread-3 INFO ResourceUtils: ==============================================================
21/07/21 19:13:17.833 Thread-3 INFO SparkContext: Submitted application: myApp
21/07/21 19:13:17.892 Thread-3 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/07/21 19:13:17.935 Thread-3 INFO ResourceProfile: Limiting resource is cpu
21/07/21 19:13:17.938 Thread-3 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/07/21 19:13:18.077 Thread-3 INFO SecurityManager: Changing view acls to: daniel
21/07/21 19:13:18.078 Thread-3 INFO SecurityManager: Changing modify acls to: daniel
21/07/21 19:13:18.078 Thread-3 INFO SecurityManager: Changing view acls groups to: 
21/07/21 19:13:18.079 Thread-3 INFO SecurityManager: Changing modify acls groups to: 
21/07/21 19:13:18.080 Thread-3 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/21 19:13:18.722 Thread-3 INFO Utils: Successfully started service 'sparkDriver' on port 41053.
21/07/21 19:13:18.795 Thread-3 INFO SparkEnv: Registering MapOutputTracker
21/07/21 19:13:18.895 Thread-3 INFO SparkEnv: Registering BlockManagerMaster
21/07/21 19:13:18.962 Thread-3 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/21 19:13:18.968 Thread-3 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/21 19:13:18.983 Thread-3 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/21 19:13:19.026 Thread-3 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6c44eff3-4ea5-4cc8-b18f-5bc3a0aa65f2
21/07/21 19:13:19.086 Thread-3 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
21/07/21 19:13:19.131 Thread-3 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/21 19:13:19.380 Thread-3 INFO log: Logging initialized @6745ms to org.sparkproject.jetty.util.log.Slf4jLog
21/07/21 19:13:19.511 Thread-3 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-Ubuntu-0ubuntu2.20.04
21/07/21 19:13:19.552 Thread-3 INFO Server: Started @6920ms
21/07/21 19:13:19.627 Thread-3 INFO AbstractConnector: Started ServerConnector@19c610dd{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
21/07/21 19:13:19.628 Thread-3 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/07/21 19:13:19.716 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7189ce14{/jobs,null,AVAILABLE,@Spark}
21/07/21 19:13:19.724 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3fe224b2{/jobs/json,null,AVAILABLE,@Spark}
21/07/21 19:13:19.737 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@297ef4cb{/jobs/job,null,AVAILABLE,@Spark}
21/07/21 19:13:19.748 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4713d007{/jobs/job/json,null,AVAILABLE,@Spark}
21/07/21 19:13:19.755 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@51a08ff0{/stages,null,AVAILABLE,@Spark}
21/07/21 19:13:19.758 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3ac84f4b{/stages/json,null,AVAILABLE,@Spark}
21/07/21 19:13:19.762 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2b3c6e38{/stages/stage,null,AVAILABLE,@Spark}
21/07/21 19:13:19.768 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@f241a92{/stages/stage/json,null,AVAILABLE,@Spark}
21/07/21 19:13:19.773 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@68b09f68{/stages/pool,null,AVAILABLE,@Spark}
21/07/21 19:13:19.776 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1e5257ce{/stages/pool/json,null,AVAILABLE,@Spark}
21/07/21 19:13:19.779 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1f87d2b6{/storage,null,AVAILABLE,@Spark}
21/07/21 19:13:19.782 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4b6dd3ba{/storage/json,null,AVAILABLE,@Spark}
21/07/21 19:13:19.786 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1bccae49{/storage/rdd,null,AVAILABLE,@Spark}
21/07/21 19:13:19.790 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@11bed026{/storage/rdd/json,null,AVAILABLE,@Spark}
21/07/21 19:13:19.793 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2b8b689c{/environment,null,AVAILABLE,@Spark}
21/07/21 19:13:19.797 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@217d4c58{/environment/json,null,AVAILABLE,@Spark}
21/07/21 19:13:19.800 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5dd1ff3e{/executors,null,AVAILABLE,@Spark}
21/07/21 19:13:19.804 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7c90d529{/executors/json,null,AVAILABLE,@Spark}
21/07/21 19:13:19.809 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@76b0fe01{/executors/threadDump,null,AVAILABLE,@Spark}
21/07/21 19:13:19.817 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@19a2f462{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/07/21 19:13:19.854 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1a46fe63{/static,null,AVAILABLE,@Spark}
21/07/21 19:13:19.858 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@825caa{/,null,AVAILABLE,@Spark}
21/07/21 19:13:19.877 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@e283b41{/api,null,AVAILABLE,@Spark}
21/07/21 19:13:19.882 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5f431549{/jobs/job/kill,null,AVAILABLE,@Spark}
21/07/21 19:13:19.897 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@76fd029{/stages/stage/kill,null,AVAILABLE,@Spark}
21/07/21 19:13:19.901 Thread-3 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.178.70:4040
21/07/21 19:13:20.688 Thread-3 INFO Executor: Starting executor ID driver on host 192.168.178.70
21/07/21 19:13:20.765 Thread-3 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43161.
21/07/21 19:13:20.765 Thread-3 INFO NettyBlockTransferService: Server created on 192.168.178.70:43161
21/07/21 19:13:20.768 Thread-3 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/21 19:13:20.786 Thread-3 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.178.70, 43161, None)
21/07/21 19:13:20.790 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager 192.168.178.70:43161 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.178.70, 43161, None)
21/07/21 19:13:20.794 Thread-3 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.178.70, 43161, None)
21/07/21 19:13:20.797 Thread-3 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.178.70, 43161, None)
21/07/21 19:13:21.207 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@692a7461{/metrics/json,null,AVAILABLE,@Spark}
21/07/21 19:13:21.844 Thread-3 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse').
21/07/21 19:13:21.846 Thread-3 INFO SharedState: Warehouse path is 'file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse'.
21/07/21 19:13:21.889 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@72d8ecba{/SQL,null,AVAILABLE,@Spark}
21/07/21 19:13:21.890 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4ae3d433{/SQL/json,null,AVAILABLE,@Spark}
21/07/21 19:13:21.892 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2d30d38c{/SQL/execution,null,AVAILABLE,@Spark}
21/07/21 19:13:21.894 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@57334630{/SQL/execution/json,null,AVAILABLE,@Spark}
21/07/21 19:13:21.919 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4d2ee1b1{/static/sql,null,AVAILABLE,@Spark}
21/07/21 19:20:31.391 stream execution thread for [id = 12fed59a-0e40-4542-8270-166aa9ce91f2, runId = 449614a7-c229-414d-8300-388c858f2e97] ERROR MicroBatchExecution: Query [id = 12fed59a-0e40-4542-8270-166aa9ce91f2, runId = 449614a7-c229-414d-8300-388c858f2e97] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.kafka010.KafkaConfigUpdater
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	... 33 more
21/07/21 19:22:47.501 stream execution thread for [id = 638d123f-a6c1-4947-81c4-beea20609750, runId = e9656da7-f326-448f-ab73-49ff037cad45] ERROR MicroBatchExecution: Query [id = 638d123f-a6c1-4947-81c4-beea20609750, runId = e9656da7-f326-448f-ab73-49ff037cad45] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/21 19:22:58.716 stream execution thread for [id = af8dd1c2-0d68-4424-9027-a0f50ac4aaf9, runId = 38387a74-7244-4dcc-8130-ac11e37aa1cd] ERROR MicroBatchExecution: Query [id = af8dd1c2-0d68-4424-9027-a0f50ac4aaf9, runId = 38387a74-7244-4dcc-8130-ac11e37aa1cd] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/21 19:23:03.547 stream execution thread for [id = 5055ac36-9cbd-419f-84c8-a69b7745365f, runId = ad895861-d4e2-4d34-b2b8-dfa152a88e05] ERROR MicroBatchExecution: Query [id = 5055ac36-9cbd-419f-84c8-a69b7745365f, runId = ad895861-d4e2-4d34-b2b8-dfa152a88e05] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/21 19:28:43.667 stream execution thread for [id = 1603a83a-1f41-4881-8ce5-c32c5b1ca808, runId = 6e42bbeb-3031-4a38-af9a-72b4f3add967] ERROR MicroBatchExecution: Query [id = 1603a83a-1f41-4881-8ce5-c32c5b1ca808, runId = 6e42bbeb-3031-4a38-af9a-72b4f3add967] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/21 19:28:47.983 stream execution thread for [id = f28569e0-0115-4484-8a6f-32322649ed76, runId = a20dcc47-86c8-47c6-b00f-12bc6716dd3c] ERROR MicroBatchExecution: Query [id = f28569e0-0115-4484-8a6f-32322649ed76, runId = a20dcc47-86c8-47c6-b00f-12bc6716dd3c] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/21 19:34:38.297 stream execution thread for [id = ac7a750d-0782-4d3a-b9e9-30b2ce60a524, runId = 853f833a-3408-4a2a-aa13-84a44740bda7] ERROR MicroBatchExecution: Query [id = ac7a750d-0782-4d3a-b9e9-30b2ce60a524, runId = 853f833a-3408-4a2a-aa13-84a44740bda7] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/21 19:35:01.358 stream execution thread for [id = efbb86fe-f5d1-4894-a02e-c3bb85b49641, runId = c69eb1b4-068c-4135-b905-5d36c790667d] ERROR MicroBatchExecution: Query [id = efbb86fe-f5d1-4894-a02e-c3bb85b49641, runId = c69eb1b4-068c-4135-b905-5d36c790667d] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/21 20:58:57.482 stream execution thread for [id = ebc33487-d586-4df3-8bdc-006cb5061366, runId = 773cb05e-ef4f-4f8d-9114-4c07a8f9c601] ERROR MicroBatchExecution: Query [id = ebc33487-d586-4df3-8bdc-006cb5061366, runId = 773cb05e-ef4f-4f8d-9114-4c07a8f9c601] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/21 20:59:32.929 stream execution thread for [id = 8d71be61-532a-406b-8d3f-57d4c5a0ea6d, runId = e3e5cbb3-f39c-44b3-a0d6-77554ec8beef] ERROR MicroBatchExecution: Query [id = 8d71be61-532a-406b-8d3f-57d4c5a0ea6d, runId = e3e5cbb3-f39c-44b3-a0d6-77554ec8beef] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/21 21:01:35.142 stream execution thread for some_name [id = 7ceaa175-70c0-497b-8a06-2a4e449eb657, runId = cf7fb05b-6547-4263-a107-a8af0186f17b] ERROR MicroBatchExecution: Query some_name [id = 7ceaa175-70c0-497b-8a06-2a4e449eb657, runId = cf7fb05b-6547-4263-a107-a8af0186f17b] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/22 08:14:07.149 main WARN Utils: Your hostname, Yoga resolves to a loopback address: 127.0.1.1; using 192.168.178.70 instead (on interface wlp4s0)
21/07/22 08:14:07.169 main WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/07/22 08:14:10.092 main WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/07/22 08:14:10.620 main INFO SecurityManager: Changing view acls to: daniel
21/07/22 08:14:10.632 main INFO SecurityManager: Changing modify acls to: daniel
21/07/22 08:14:10.641 main INFO SecurityManager: Changing view acls groups to: 
21/07/22 08:14:10.647 main INFO SecurityManager: Changing modify acls groups to: 
21/07/22 08:14:10.649 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/22 08:14:12.305 Thread-3 INFO SparkContext: Running Spark version 3.1.2
21/07/22 08:14:12.533 Thread-3 INFO ResourceUtils: ==============================================================
21/07/22 08:14:12.540 Thread-3 INFO ResourceUtils: No custom resources configured for spark.driver.
21/07/22 08:14:12.542 Thread-3 INFO ResourceUtils: ==============================================================
21/07/22 08:14:12.548 Thread-3 INFO SparkContext: Submitted application: myApp
21/07/22 08:14:13.060 Thread-3 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/07/22 08:14:13.228 Thread-3 INFO ResourceProfile: Limiting resource is cpu
21/07/22 08:14:13.235 Thread-3 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/07/22 08:14:13.414 Thread-3 INFO SecurityManager: Changing view acls to: daniel
21/07/22 08:14:13.418 Thread-3 INFO SecurityManager: Changing modify acls to: daniel
21/07/22 08:14:13.418 Thread-3 INFO SecurityManager: Changing view acls groups to: 
21/07/22 08:14:13.419 Thread-3 INFO SecurityManager: Changing modify acls groups to: 
21/07/22 08:14:13.419 Thread-3 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(daniel); groups with view permissions: Set(); users  with modify permissions: Set(daniel); groups with modify permissions: Set()
21/07/22 08:14:15.222 Thread-3 INFO Utils: Successfully started service 'sparkDriver' on port 44183.
21/07/22 08:14:15.382 Thread-3 INFO SparkEnv: Registering MapOutputTracker
21/07/22 08:14:15.642 Thread-3 INFO SparkEnv: Registering BlockManagerMaster
21/07/22 08:14:15.783 Thread-3 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/22 08:14:15.784 Thread-3 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/22 08:14:15.821 Thread-3 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/22 08:14:15.896 Thread-3 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a3cb9591-0e00-4747-ac39-c93780f93d83
21/07/22 08:14:15.977 Thread-3 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
21/07/22 08:14:16.031 Thread-3 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/22 08:14:16.695 Thread-3 INFO log: Logging initialized @19182ms to org.sparkproject.jetty.util.log.Slf4jLog
21/07/22 08:14:17.101 Thread-3 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-Ubuntu-0ubuntu2.20.04
21/07/22 08:14:17.193 Thread-3 INFO Server: Started @19684ms
21/07/22 08:14:17.335 Thread-3 INFO AbstractConnector: Started ServerConnector@2eadcba1{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
21/07/22 08:14:17.336 Thread-3 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/07/22 08:14:17.455 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@18bd25d{/jobs,null,AVAILABLE,@Spark}
21/07/22 08:14:17.459 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5b7b61c9{/jobs/json,null,AVAILABLE,@Spark}
21/07/22 08:14:17.461 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6791ecc8{/jobs/job,null,AVAILABLE,@Spark}
21/07/22 08:14:17.481 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@458522e3{/jobs/job/json,null,AVAILABLE,@Spark}
21/07/22 08:14:17.483 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4f19d1f0{/stages,null,AVAILABLE,@Spark}
21/07/22 08:14:17.485 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@60a618c9{/stages/json,null,AVAILABLE,@Spark}
21/07/22 08:14:17.488 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2841b357{/stages/stage,null,AVAILABLE,@Spark}
21/07/22 08:14:17.501 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5fd4c468{/stages/stage/json,null,AVAILABLE,@Spark}
21/07/22 08:14:17.506 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@cd005ca{/stages/pool,null,AVAILABLE,@Spark}
21/07/22 08:14:17.516 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@78513f9f{/stages/pool/json,null,AVAILABLE,@Spark}
21/07/22 08:14:17.521 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5eff2d7c{/storage,null,AVAILABLE,@Spark}
21/07/22 08:14:17.523 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@54e5e988{/storage/json,null,AVAILABLE,@Spark}
21/07/22 08:14:17.533 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4d8043cd{/storage/rdd,null,AVAILABLE,@Spark}
21/07/22 08:14:17.537 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5f39e389{/storage/rdd/json,null,AVAILABLE,@Spark}
21/07/22 08:14:17.540 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6210854d{/environment,null,AVAILABLE,@Spark}
21/07/22 08:14:17.548 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@90c99e0{/environment/json,null,AVAILABLE,@Spark}
21/07/22 08:14:17.550 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4b1d0e67{/executors,null,AVAILABLE,@Spark}
21/07/22 08:14:17.552 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@777c6cfe{/executors/json,null,AVAILABLE,@Spark}
21/07/22 08:14:17.555 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@77303614{/executors/threadDump,null,AVAILABLE,@Spark}
21/07/22 08:14:17.575 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@68480188{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/07/22 08:14:17.603 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@215aef98{/static,null,AVAILABLE,@Spark}
21/07/22 08:14:17.606 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@701ab0d5{/,null,AVAILABLE,@Spark}
21/07/22 08:14:17.613 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@781f9b06{/api,null,AVAILABLE,@Spark}
21/07/22 08:14:17.621 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4acaf3b1{/jobs/job/kill,null,AVAILABLE,@Spark}
21/07/22 08:14:17.624 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@57c24406{/stages/stage/kill,null,AVAILABLE,@Spark}
21/07/22 08:14:17.634 Thread-3 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://Yoga.fritz.box:4040
21/07/22 08:14:18.958 Thread-3 INFO Executor: Starting executor ID driver on host Yoga.fritz.box
21/07/22 08:14:19.101 Thread-3 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40843.
21/07/22 08:14:19.101 Thread-3 INFO NettyBlockTransferService: Server created on Yoga.fritz.box:40843
21/07/22 08:14:19.105 Thread-3 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/22 08:14:19.148 Thread-3 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, Yoga.fritz.box, 40843, None)
21/07/22 08:14:19.156 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager Yoga.fritz.box:40843 with 434.4 MiB RAM, BlockManagerId(driver, Yoga.fritz.box, 40843, None)
21/07/22 08:14:19.185 Thread-3 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, Yoga.fritz.box, 40843, None)
21/07/22 08:14:19.214 Thread-3 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, Yoga.fritz.box, 40843, None)
21/07/22 08:14:20.602 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@18887d17{/metrics/json,null,AVAILABLE,@Spark}
21/07/22 08:14:22.545 Thread-3 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse').
21/07/22 08:14:22.556 Thread-3 INFO SharedState: Warehouse path is 'file:/home/daniel/Schreibtisch/Projekte/db/spark-docker/spark-warehouse'.
21/07/22 08:14:22.650 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@55ccca34{/SQL,null,AVAILABLE,@Spark}
21/07/22 08:14:22.684 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@441f6bf9{/SQL/json,null,AVAILABLE,@Spark}
21/07/22 08:14:22.701 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2a21d39f{/SQL/execution,null,AVAILABLE,@Spark}
21/07/22 08:14:22.716 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@57455f84{/SQL/execution/json,null,AVAILABLE,@Spark}
21/07/22 08:14:22.827 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@23d5b0f2{/static/sql,null,AVAILABLE,@Spark}
21/07/22 08:15:50.381 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Substitution after 1 iterations.
21/07/22 08:15:50.383 Thread-3 TRACE PlanChangeLogger: Batch Substitution has no effect.
21/07/22 08:15:50.383 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Disable Hints after 1 iterations.
21/07/22 08:15:50.383 Thread-3 TRACE PlanChangeLogger: Batch Disable Hints has no effect.
21/07/22 08:15:50.387 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Hints after 1 iterations.
21/07/22 08:15:50.387 Thread-3 TRACE PlanChangeLogger: Batch Hints has no effect.
21/07/22 08:15:50.388 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Simple Sanity Check after 1 iterations.
21/07/22 08:15:50.388 Thread-3 TRACE PlanChangeLogger: Batch Simple Sanity Check has no effect.
21/07/22 08:15:50.469 Thread-3 TRACE Analyzer$ResolveReferences: Attempting to resolve StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
21/07/22 08:15:50.634 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Resolution after 1 iterations.
21/07/22 08:15:50.635 Thread-3 TRACE PlanChangeLogger: Batch Resolution has no effect.
21/07/22 08:15:50.637 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Apply Char Padding after 1 iterations.
21/07/22 08:15:50.637 Thread-3 TRACE PlanChangeLogger: Batch Apply Char Padding has no effect.
21/07/22 08:15:50.655 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Post-Hoc Resolution after 1 iterations.
21/07/22 08:15:50.655 Thread-3 TRACE PlanChangeLogger: Batch Post-Hoc Resolution has no effect.
21/07/22 08:15:50.657 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Normalize Alter Table after 1 iterations.
21/07/22 08:15:50.657 Thread-3 TRACE PlanChangeLogger: Batch Normalize Alter Table has no effect.
21/07/22 08:15:50.658 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove Unresolved Hints after 1 iterations.
21/07/22 08:15:50.658 Thread-3 TRACE PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
21/07/22 08:15:50.663 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Nondeterministic after 1 iterations.
21/07/22 08:15:50.663 Thread-3 TRACE PlanChangeLogger: Batch Nondeterministic has no effect.
21/07/22 08:15:50.667 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UDF after 1 iterations.
21/07/22 08:15:50.668 Thread-3 TRACE PlanChangeLogger: Batch UDF has no effect.
21/07/22 08:15:50.669 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UpdateNullability after 1 iterations.
21/07/22 08:15:50.669 Thread-3 TRACE PlanChangeLogger: Batch UpdateNullability has no effect.
21/07/22 08:15:50.670 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Subquery after 1 iterations.
21/07/22 08:15:50.671 Thread-3 TRACE PlanChangeLogger: Batch Subquery has no effect.
21/07/22 08:15:50.674 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Cleanup after 1 iterations.
21/07/22 08:15:50.675 Thread-3 TRACE PlanChangeLogger: Batch Cleanup has no effect.
21/07/22 08:15:50.677 Thread-3 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 86
Total time: 0.311381341 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/07/22 08:15:57.659 Thread-3 DEBUG SparkSqlParser: Parsing command: CAST(key AS STRING)
21/07/22 08:15:57.961 Thread-3 DEBUG SparkSqlParser: Parsing command: CAST(value AS STRING)
21/07/22 08:15:57.972 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Substitution after 1 iterations.
21/07/22 08:15:57.972 Thread-3 TRACE PlanChangeLogger: Batch Substitution has no effect.
21/07/22 08:15:57.973 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Disable Hints after 1 iterations.
21/07/22 08:15:57.973 Thread-3 TRACE PlanChangeLogger: Batch Disable Hints has no effect.
21/07/22 08:15:57.974 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Hints after 1 iterations.
21/07/22 08:15:57.976 Thread-3 TRACE PlanChangeLogger: Batch Hints has no effect.
21/07/22 08:15:57.976 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Simple Sanity Check after 1 iterations.
21/07/22 08:15:57.977 Thread-3 TRACE PlanChangeLogger: Batch Simple Sanity Check has no effect.
21/07/22 08:15:57.984 Thread-3 TRACE Analyzer$ResolveReferences: Attempting to resolve 'Project [unresolvedalias(cast('key as string), None), unresolvedalias(cast('value as string), None)]
21/07/22 08:15:57.997 Thread-3 DEBUG Analyzer$ResolveReferences: Resolving 'key to key#7
21/07/22 08:15:57.998 Thread-3 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#8
21/07/22 08:15:58.019 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences ===
!'Project [unresolvedalias(cast('key as string), None), unresolvedalias(cast('value as string), None)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         'Project [unresolvedalias(cast(key#7 as string), None), unresolvedalias(cast(value#8 as string), None)]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
           
21/07/22 08:15:58.046 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases ===
!'Project [unresolvedalias(cast(key#7 as string), None), unresolvedalias(cast(value#8 as string), None)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
           
21/07/22 08:15:58.070 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.ResolveTimeZone ===
 Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
           
21/07/22 08:15:58.080 Thread-3 TRACE Analyzer$ResolveReferences: Attempting to resolve Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]
21/07/22 08:15:58.086 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Resolution after 2 iterations.
21/07/22 08:15:58.094 Thread-3 TRACE PlanChangeLogger: 
=== Result of Batch Resolution ===
!'Project [unresolvedalias(cast('key as string), None), unresolvedalias(cast('value as string), None)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
          
21/07/22 08:15:58.095 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Apply Char Padding after 1 iterations.
21/07/22 08:15:58.096 Thread-3 TRACE PlanChangeLogger: Batch Apply Char Padding has no effect.
21/07/22 08:15:58.098 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Post-Hoc Resolution after 1 iterations.
21/07/22 08:15:58.098 Thread-3 TRACE PlanChangeLogger: Batch Post-Hoc Resolution has no effect.
21/07/22 08:15:58.099 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Normalize Alter Table after 1 iterations.
21/07/22 08:15:58.099 Thread-3 TRACE PlanChangeLogger: Batch Normalize Alter Table has no effect.
21/07/22 08:15:58.099 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove Unresolved Hints after 1 iterations.
21/07/22 08:15:58.099 Thread-3 TRACE PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
21/07/22 08:15:58.100 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Nondeterministic after 1 iterations.
21/07/22 08:15:58.100 Thread-3 TRACE PlanChangeLogger: Batch Nondeterministic has no effect.
21/07/22 08:15:58.100 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UDF after 1 iterations.
21/07/22 08:15:58.101 Thread-3 TRACE PlanChangeLogger: Batch UDF has no effect.
21/07/22 08:15:58.105 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UpdateNullability after 1 iterations.
21/07/22 08:15:58.105 Thread-3 TRACE PlanChangeLogger: Batch UpdateNullability has no effect.
21/07/22 08:15:58.105 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Subquery after 1 iterations.
21/07/22 08:15:58.105 Thread-3 TRACE PlanChangeLogger: Batch Subquery has no effect.
21/07/22 08:15:58.114 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.CleanupAliases ===
 Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
           
21/07/22 08:15:58.115 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Cleanup after 2 iterations.
21/07/22 08:15:58.122 Thread-3 TRACE PlanChangeLogger: 
=== Result of Batch Cleanup ===
 Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
          
21/07/22 08:15:58.123 Thread-3 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 150
Total time: 0.076858983 seconds
Total number of effective runs: 4
Total time of effective runs: 0.026236154 seconds
      
21/07/22 08:16:02.640 Thread-3 DEBUG SparkSqlParser: Parsing command: CAST(key AS STRING)
21/07/22 08:16:02.642 Thread-3 DEBUG SparkSqlParser: Parsing command: CAST(value AS STRING)
21/07/22 08:16:02.647 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Substitution after 1 iterations.
21/07/22 08:16:02.647 Thread-3 TRACE PlanChangeLogger: Batch Substitution has no effect.
21/07/22 08:16:02.647 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Disable Hints after 1 iterations.
21/07/22 08:16:02.647 Thread-3 TRACE PlanChangeLogger: Batch Disable Hints has no effect.
21/07/22 08:16:02.648 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Hints after 1 iterations.
21/07/22 08:16:02.648 Thread-3 TRACE PlanChangeLogger: Batch Hints has no effect.
21/07/22 08:16:02.648 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Simple Sanity Check after 1 iterations.
21/07/22 08:16:02.648 Thread-3 TRACE PlanChangeLogger: Batch Simple Sanity Check has no effect.
21/07/22 08:16:02.650 Thread-3 TRACE Analyzer$ResolveReferences: Attempting to resolve 'Project [unresolvedalias(cast('key as string), None), unresolvedalias(cast('value as string), None)]
21/07/22 08:16:02.650 Thread-3 DEBUG Analyzer$ResolveReferences: Resolving 'key to key#7
21/07/22 08:16:02.651 Thread-3 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#8
21/07/22 08:16:02.658 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences ===
!'Project [unresolvedalias(cast('key as string), None), unresolvedalias(cast('value as string), None)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         'Project [unresolvedalias(cast(key#7 as string), None), unresolvedalias(cast(value#8 as string), None)]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
           
21/07/22 08:16:02.665 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases ===
!'Project [unresolvedalias(cast(key#7 as string), None), unresolvedalias(cast(value#8 as string), None)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Project [cast(key#7 as string) AS key#25, cast(value#8 as string) AS value#26]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
           
21/07/22 08:16:02.673 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.ResolveTimeZone ===
 Project [cast(key#7 as string) AS key#25, cast(value#8 as string) AS value#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Project [cast(key#7 as string) AS key#25, cast(value#8 as string) AS value#26]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
           
21/07/22 08:16:02.677 Thread-3 TRACE Analyzer$ResolveReferences: Attempting to resolve Project [cast(key#7 as string) AS key#25, cast(value#8 as string) AS value#26]
21/07/22 08:16:02.682 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Resolution after 2 iterations.
21/07/22 08:16:02.691 Thread-3 TRACE PlanChangeLogger: 
=== Result of Batch Resolution ===
!'Project [unresolvedalias(cast('key as string), None), unresolvedalias(cast('value as string), None)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Project [cast(key#7 as string) AS key#25, cast(value#8 as string) AS value#26]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
          
21/07/22 08:16:02.693 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Apply Char Padding after 1 iterations.
21/07/22 08:16:02.693 Thread-3 TRACE PlanChangeLogger: Batch Apply Char Padding has no effect.
21/07/22 08:16:02.694 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Post-Hoc Resolution after 1 iterations.
21/07/22 08:16:02.694 Thread-3 TRACE PlanChangeLogger: Batch Post-Hoc Resolution has no effect.
21/07/22 08:16:02.694 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Normalize Alter Table after 1 iterations.
21/07/22 08:16:02.694 Thread-3 TRACE PlanChangeLogger: Batch Normalize Alter Table has no effect.
21/07/22 08:16:02.695 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove Unresolved Hints after 1 iterations.
21/07/22 08:16:02.695 Thread-3 TRACE PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
21/07/22 08:16:02.695 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Nondeterministic after 1 iterations.
21/07/22 08:16:02.697 Thread-3 TRACE PlanChangeLogger: Batch Nondeterministic has no effect.
21/07/22 08:16:02.698 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UDF after 1 iterations.
21/07/22 08:16:02.699 Thread-3 TRACE PlanChangeLogger: Batch UDF has no effect.
21/07/22 08:16:02.701 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UpdateNullability after 1 iterations.
21/07/22 08:16:02.701 Thread-3 TRACE PlanChangeLogger: Batch UpdateNullability has no effect.
21/07/22 08:16:02.701 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Subquery after 1 iterations.
21/07/22 08:16:02.701 Thread-3 TRACE PlanChangeLogger: Batch Subquery has no effect.
21/07/22 08:16:02.722 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.CleanupAliases ===
 Project [cast(key#7 as string) AS key#25, cast(value#8 as string) AS value#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Project [cast(key#7 as string) AS key#25, cast(value#8 as string) AS value#26]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
           
21/07/22 08:16:02.724 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Cleanup after 2 iterations.
21/07/22 08:16:02.730 Thread-3 TRACE PlanChangeLogger: 
=== Result of Batch Cleanup ===
 Project [cast(key#7 as string) AS key#25, cast(value#8 as string) AS value#26]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Project [cast(key#7 as string) AS key#25, cast(value#8 as string) AS value#26]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05, [kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> changes),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]
          
21/07/22 08:16:02.731 Thread-3 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 150
Total time: 0.019754292 seconds
Total number of effective runs: 4
Total time of effective runs: 0.004407945 seconds
      
21/07/22 08:16:02.798 Thread-3 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
21/07/22 08:16:02.823 Thread-3 DEBUG StreamingQueryStatisticsPage: Supported custom metrics: List(StateStoreCustomSizeMetric(stateOnCurrentVersionSizeBytes,estimated size of state only on current version), StateStoreCustomSumMetric(loadedMapCacheHitCount,count of cache hit on states cache in provider), StateStoreCustomSumMetric(loadedMapCacheMissCount,count of cache miss on states cache in provider))
21/07/22 08:16:02.825 Thread-3 DEBUG StreamingQueryStatisticsPage: Enabled custom metrics: ArrayBuffer()
21/07/22 08:16:02.825 Thread-3 DEBUG DecoratedObjectFactory: Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@3f2ba94f
21/07/22 08:16:02.825 Thread-3 DEBUG ContainerLifeCycle: o.s.j.s.ServletContextHandler@225c5550{/,null,STOPPED} added {ServletHandler@31b3f9c3{STOPPED},MANAGED}
21/07/22 08:16:02.826 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@31b3f9c3{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$1-4c643e3e==org.apache.spark.ui.JettyUtils$$anon$1@863c7b7f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
21/07/22 08:16:02.827 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@31b3f9c3{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$1-4c643e3e,POJO}
21/07/22 08:16:02.827 Thread-3 DEBUG DecoratedObjectFactory: Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@2e67972c
21/07/22 08:16:02.828 Thread-3 DEBUG ContainerLifeCycle: o.s.j.s.ServletContextHandler@6e51a165{/,null,STOPPED} added {ServletHandler@4082308d{STOPPED},MANAGED}
21/07/22 08:16:02.828 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@4082308d{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$1-2dfa736d==org.apache.spark.ui.JettyUtils$$anon$1@3fc432a5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
21/07/22 08:16:02.829 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@4082308d{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$1-2dfa736d,POJO}
21/07/22 08:16:02.830 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@31b3f9c3{STOPPED} added {org.apache.spark.ui.HttpSecurityFilter-3fb65f09==org.apache.spark.ui.HttpSecurityFilter@3fb65f09{inst=false,async=true,src=EMBEDDED:null},AUTO}
21/07/22 08:16:02.837 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@31b3f9c3{STOPPED} added {[/*]/[]/[ERROR, ASYNC, INCLUDE, FORWARD, REQUEST]=>org.apache.spark.ui.HttpSecurityFilter-3fb65f09,POJO}
21/07/22 08:16:02.838 Thread-3 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
21/07/22 08:16:02.839 Thread-3 DEBUG GzipHandler: GzipHandler@33b27550{STOPPED,min=32,inflate=-1} mime types IncludeExclude@2ca5469{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
21/07/22 08:16:02.839 Thread-3 DEBUG ContainerLifeCycle: GzipHandler@33b27550{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@225c5550{/StreamingQuery,null,STOPPED,@Spark},MANAGED}
21/07/22 08:16:02.840 Thread-3 DEBUG ContextHandlerCollection: ->[{GzipHandler@2defb1ab{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@701ab0d5{/,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.841 Thread-3 DEBUG ContextHandlerCollection: storage/rdd->[{GzipHandler@6706c0d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4d8043cd{/storage/rdd,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.841 Thread-3 DEBUG ContextHandlerCollection: storage->[{GzipHandler@26c88166{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5eff2d7c{/storage,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.842 Thread-3 DEBUG ContextHandlerCollection: storage/rdd/json->[{GzipHandler@7671e0ca{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5f39e389{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.842 Thread-3 DEBUG ContextHandlerCollection: SQL/execution/json->[{GzipHandler@26436546{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@57455f84{/SQL/execution/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.843 Thread-3 DEBUG ContextHandlerCollection: api->[{GzipHandler@23d67587{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@781f9b06{/api,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.843 Thread-3 DEBUG ContextHandlerCollection: stages/pool/json->[{GzipHandler@41f7132a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@78513f9f{/stages/pool/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.844 Thread-3 DEBUG ContextHandlerCollection: stages/pool->[{GzipHandler@3879dc6e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@cd005ca{/stages/pool,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.844 Thread-3 DEBUG ContextHandlerCollection: jobs/json->[{GzipHandler@52df772{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5b7b61c9{/jobs/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.845 Thread-3 DEBUG ContextHandlerCollection: static->[{GzipHandler@1547d01d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@215aef98{/static,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.845 Thread-3 DEBUG ContextHandlerCollection: executors/json->[{GzipHandler@6cd0efdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@777c6cfe{/executors/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.846 Thread-3 DEBUG ContextHandlerCollection: stages/stage/json->[{GzipHandler@1a3cd484{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fd4c468{/stages/stage/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.846 Thread-3 DEBUG ContextHandlerCollection: executors/threadDump/json->[{GzipHandler@704026d6{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@68480188{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.846 Thread-3 DEBUG ContextHandlerCollection: environment/json->[{GzipHandler@5b2cc7fb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@90c99e0{/environment/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.847 Thread-3 DEBUG ContextHandlerCollection: jobs/job/json->[{GzipHandler@30696d4d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@458522e3{/jobs/job/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.847 Thread-3 DEBUG ContextHandlerCollection: jobs->[{GzipHandler@4c0d1730{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@18bd25d{/jobs,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.847 Thread-3 DEBUG ContextHandlerCollection: stages/json->[{GzipHandler@3d8afef1{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@60a618c9{/stages/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.847 Thread-3 DEBUG ContextHandlerCollection: stages/stage->[{GzipHandler@a4e5a4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2841b357{/stages/stage,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.848 Thread-3 DEBUG ContextHandlerCollection: storage/json->[{GzipHandler@7b5cf3f8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@54e5e988{/storage/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.848 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery->[{GzipHandler@33b27550{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@225c5550{/StreamingQuery,null,STOPPED,@Spark}]}]
21/07/22 08:16:02.848 Thread-3 DEBUG ContextHandlerCollection: SQL->[{GzipHandler@5fccdf34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@55ccca34{/SQL,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.849 Thread-3 DEBUG ContextHandlerCollection: static/sql->[{GzipHandler@355bdf4a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@23d5b0f2{/static/sql,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.849 Thread-3 DEBUG ContextHandlerCollection: stages/stage/kill->[{GzipHandler@12758580{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@57c24406{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.849 Thread-3 DEBUG ContextHandlerCollection: jobs/job->[{GzipHandler@e2a7705{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6791ecc8{/jobs/job,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.850 Thread-3 DEBUG ContextHandlerCollection: environment->[{GzipHandler@37262c8b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6210854d{/environment,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.850 Thread-3 DEBUG ContextHandlerCollection: stages->[{GzipHandler@5cec14aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4f19d1f0{/stages,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.850 Thread-3 DEBUG ContextHandlerCollection: executors->[{GzipHandler@620bd332{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4b1d0e67{/executors,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.851 Thread-3 DEBUG ContextHandlerCollection: SQL/json->[{GzipHandler@3f441229{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@441f6bf9{/SQL/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.851 Thread-3 DEBUG ContextHandlerCollection: jobs/job/kill->[{GzipHandler@3980333e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4acaf3b1{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.852 Thread-3 DEBUG ContextHandlerCollection: metrics/json->[{GzipHandler@57539e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@18887d17{/metrics/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.852 Thread-3 DEBUG ContextHandlerCollection: SQL/execution->[{GzipHandler@39dbf83f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2a21d39f{/SQL/execution,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.852 Thread-3 DEBUG ContextHandlerCollection: executors/threadDump->[{GzipHandler@26916077{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77303614{/executors/threadDump,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.853 Thread-3 DEBUG ContainerLifeCycle: ContextHandlerCollection@1eb0be58{STARTED} added {GzipHandler@33b27550{STOPPED,min=32,inflate=-1},UNMANAGED}
21/07/22 08:16:02.853 Thread-3 DEBUG AbstractLifeCycle: starting o.s.j.s.ServletContextHandler@225c5550{/StreamingQuery,null,STOPPED,@Spark}
21/07/22 08:16:02.853 Thread-3 DEBUG AbstractHandler: starting o.s.j.s.ServletContextHandler@225c5550{/StreamingQuery,null,STARTING,@Spark}
21/07/22 08:16:02.854 Thread-3 DEBUG AbstractLifeCycle: starting ServletHandler@31b3f9c3{STOPPED}
21/07/22 08:16:02.854 Thread-3 DEBUG ServletHandler: Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-4c643e3e[EMBEDDED:null]
21/07/22 08:16:02.855 Thread-3 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$1-4c643e3e==org.apache.spark.ui.JettyUtils$$anon$1@863c7b7f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
21/07/22 08:16:02.855 Thread-3 DEBUG ServletHandler: filterNameMap={org.apache.spark.ui.HttpSecurityFilter-3fb65f09=org.apache.spark.ui.HttpSecurityFilter-3fb65f09==org.apache.spark.ui.HttpSecurityFilter@3fb65f09{inst=false,async=true,src=EMBEDDED:null}}
21/07/22 08:16:02.855 Thread-3 DEBUG ServletHandler: pathFilters=[[/*]/[]/[ERROR, ASYNC, INCLUDE, FORWARD, REQUEST]=>org.apache.spark.ui.HttpSecurityFilter-3fb65f09]
21/07/22 08:16:02.856 Thread-3 DEBUG ServletHandler: servletFilterMap={}
21/07/22 08:16:02.856 Thread-3 DEBUG ServletHandler: servletPathMap=PathMappings[size=1]
21/07/22 08:16:02.856 Thread-3 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-4c643e3e=org.apache.spark.ui.JettyUtils$$anon$1-4c643e3e==org.apache.spark.ui.JettyUtils$$anon$1@863c7b7f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
21/07/22 08:16:02.857 Thread-3 DEBUG AbstractHandler: starting ServletHandler@31b3f9c3{STARTING}
21/07/22 08:16:02.857 Thread-3 DEBUG AbstractLifeCycle: STARTED @125348ms ServletHandler@31b3f9c3{STARTED}
21/07/22 08:16:02.857 Thread-3 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.HttpSecurityFilter-3fb65f09==org.apache.spark.ui.HttpSecurityFilter@3fb65f09{inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.858 Thread-3 DEBUG AbstractLifeCycle: STARTED @125348ms org.apache.spark.ui.HttpSecurityFilter-3fb65f09==org.apache.spark.ui.HttpSecurityFilter@3fb65f09{inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.858 Thread-3 DEBUG FilterHolder: Filter.init org.apache.spark.ui.HttpSecurityFilter@32263e57
21/07/22 08:16:02.858 Thread-3 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$1-4c643e3e==org.apache.spark.ui.JettyUtils$$anon$1@863c7b7f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.859 Thread-3 DEBUG AbstractLifeCycle: STARTED @125349ms org.apache.spark.ui.JettyUtils$$anon$1-4c643e3e==org.apache.spark.ui.JettyUtils$$anon$1@863c7b7f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.859 Thread-3 DEBUG ServletHolder: Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$1-4c643e3e
21/07/22 08:16:02.859 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@225c5550{/StreamingQuery,null,AVAILABLE,@Spark}
21/07/22 08:16:02.859 Thread-3 DEBUG AbstractLifeCycle: STARTED @125350ms o.s.j.s.ServletContextHandler@225c5550{/StreamingQuery,null,AVAILABLE,@Spark}
21/07/22 08:16:02.860 Thread-3 DEBUG AbstractLifeCycle: starting GzipHandler@33b27550{STOPPED,min=32,inflate=-1}
21/07/22 08:16:02.860 Thread-3 DEBUG ContainerLifeCycle: GzipHandler@33b27550{STARTING,min=32,inflate=-1} added {DeflaterPool@545a222c{STOPPED,size=0,capacity=UNLIMITED},AUTO}
21/07/22 08:16:02.861 Thread-3 DEBUG AbstractHandler: starting GzipHandler@33b27550{STARTING,min=32,inflate=-1}
21/07/22 08:16:02.861 Thread-3 DEBUG AbstractLifeCycle: starting DeflaterPool@545a222c{STOPPED,size=0,capacity=UNLIMITED}
21/07/22 08:16:02.861 Thread-3 DEBUG AbstractLifeCycle: STARTED @125352ms DeflaterPool@545a222c{STARTED,size=0,capacity=UNLIMITED}
21/07/22 08:16:02.861 Thread-3 DEBUG AbstractLifeCycle: STARTED @125352ms GzipHandler@33b27550{STARTED,min=32,inflate=-1}
21/07/22 08:16:02.862 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@4082308d{STOPPED} added {org.apache.spark.ui.HttpSecurityFilter-79656208==org.apache.spark.ui.HttpSecurityFilter@79656208{inst=false,async=true,src=EMBEDDED:null},AUTO}
21/07/22 08:16:02.863 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@4082308d{STOPPED} added {[/*]/[]/[ERROR, ASYNC, INCLUDE, FORWARD, REQUEST]=>org.apache.spark.ui.HttpSecurityFilter-79656208,POJO}
21/07/22 08:16:02.863 Thread-3 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
21/07/22 08:16:02.864 Thread-3 DEBUG GzipHandler: GzipHandler@62bc4102{STOPPED,min=32,inflate=-1} mime types IncludeExclude@616b1f84{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
21/07/22 08:16:02.864 Thread-3 DEBUG ContainerLifeCycle: GzipHandler@62bc4102{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@6e51a165{/StreamingQuery/json,null,STOPPED,@Spark},MANAGED}
21/07/22 08:16:02.865 Thread-3 DEBUG ContextHandlerCollection: ->[{GzipHandler@2defb1ab{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@701ab0d5{/,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.869 Thread-3 DEBUG ContextHandlerCollection: storage/rdd->[{GzipHandler@6706c0d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4d8043cd{/storage/rdd,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.870 Thread-3 DEBUG ContextHandlerCollection: storage->[{GzipHandler@26c88166{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5eff2d7c{/storage,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.870 Thread-3 DEBUG ContextHandlerCollection: storage/rdd/json->[{GzipHandler@7671e0ca{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5f39e389{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.870 Thread-3 DEBUG ContextHandlerCollection: SQL/execution/json->[{GzipHandler@26436546{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@57455f84{/SQL/execution/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.871 Thread-3 DEBUG ContextHandlerCollection: api->[{GzipHandler@23d67587{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@781f9b06{/api,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.871 Thread-3 DEBUG ContextHandlerCollection: stages/pool/json->[{GzipHandler@41f7132a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@78513f9f{/stages/pool/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.871 Thread-3 DEBUG ContextHandlerCollection: stages/pool->[{GzipHandler@3879dc6e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@cd005ca{/stages/pool,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.871 Thread-3 DEBUG ContextHandlerCollection: jobs/json->[{GzipHandler@52df772{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5b7b61c9{/jobs/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.872 Thread-3 DEBUG ContextHandlerCollection: static->[{GzipHandler@1547d01d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@215aef98{/static,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.872 Thread-3 DEBUG ContextHandlerCollection: executors/json->[{GzipHandler@6cd0efdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@777c6cfe{/executors/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.872 Thread-3 DEBUG ContextHandlerCollection: stages/stage/json->[{GzipHandler@1a3cd484{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fd4c468{/stages/stage/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.877 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery/json->[{GzipHandler@62bc4102{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e51a165{/StreamingQuery/json,null,STOPPED,@Spark}]}]
21/07/22 08:16:02.878 Thread-3 DEBUG ContextHandlerCollection: executors/threadDump/json->[{GzipHandler@704026d6{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@68480188{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.878 Thread-3 DEBUG ContextHandlerCollection: environment/json->[{GzipHandler@5b2cc7fb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@90c99e0{/environment/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.879 Thread-3 DEBUG ContextHandlerCollection: jobs/job/json->[{GzipHandler@30696d4d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@458522e3{/jobs/job/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.879 Thread-3 DEBUG ContextHandlerCollection: jobs->[{GzipHandler@4c0d1730{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@18bd25d{/jobs,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.880 Thread-3 DEBUG ContextHandlerCollection: stages/json->[{GzipHandler@3d8afef1{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@60a618c9{/stages/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.880 Thread-3 DEBUG ContextHandlerCollection: stages/stage->[{GzipHandler@a4e5a4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2841b357{/stages/stage,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.880 Thread-3 DEBUG ContextHandlerCollection: storage/json->[{GzipHandler@7b5cf3f8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@54e5e988{/storage/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.881 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery->[{GzipHandler@33b27550{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@225c5550{/StreamingQuery,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.881 Thread-3 DEBUG ContextHandlerCollection: SQL->[{GzipHandler@5fccdf34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@55ccca34{/SQL,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.881 Thread-3 DEBUG ContextHandlerCollection: static/sql->[{GzipHandler@355bdf4a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@23d5b0f2{/static/sql,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.882 Thread-3 DEBUG ContextHandlerCollection: stages/stage/kill->[{GzipHandler@12758580{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@57c24406{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.882 Thread-3 DEBUG ContextHandlerCollection: jobs/job->[{GzipHandler@e2a7705{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6791ecc8{/jobs/job,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.882 Thread-3 DEBUG ContextHandlerCollection: environment->[{GzipHandler@37262c8b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6210854d{/environment,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.882 Thread-3 DEBUG ContextHandlerCollection: stages->[{GzipHandler@5cec14aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4f19d1f0{/stages,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.882 Thread-3 DEBUG ContextHandlerCollection: executors->[{GzipHandler@620bd332{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4b1d0e67{/executors,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.883 Thread-3 DEBUG ContextHandlerCollection: SQL/json->[{GzipHandler@3f441229{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@441f6bf9{/SQL/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.883 Thread-3 DEBUG ContextHandlerCollection: jobs/job/kill->[{GzipHandler@3980333e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4acaf3b1{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.883 Thread-3 DEBUG ContextHandlerCollection: metrics/json->[{GzipHandler@57539e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@18887d17{/metrics/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.885 Thread-3 DEBUG ContextHandlerCollection: SQL/execution->[{GzipHandler@39dbf83f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2a21d39f{/SQL/execution,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.885 Thread-3 DEBUG ContextHandlerCollection: executors/threadDump->[{GzipHandler@26916077{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77303614{/executors/threadDump,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.885 Thread-3 DEBUG ContainerLifeCycle: ContextHandlerCollection@1eb0be58{STARTED} added {GzipHandler@62bc4102{STOPPED,min=32,inflate=-1},UNMANAGED}
21/07/22 08:16:02.886 Thread-3 DEBUG AbstractLifeCycle: starting o.s.j.s.ServletContextHandler@6e51a165{/StreamingQuery/json,null,STOPPED,@Spark}
21/07/22 08:16:02.886 Thread-3 DEBUG AbstractHandler: starting o.s.j.s.ServletContextHandler@6e51a165{/StreamingQuery/json,null,STARTING,@Spark}
21/07/22 08:16:02.887 Thread-3 DEBUG AbstractLifeCycle: starting ServletHandler@4082308d{STOPPED}
21/07/22 08:16:02.887 Thread-3 DEBUG ServletHandler: Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-2dfa736d[EMBEDDED:null]
21/07/22 08:16:02.887 Thread-3 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$1-2dfa736d==org.apache.spark.ui.JettyUtils$$anon$1@3fc432a5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
21/07/22 08:16:02.888 Thread-3 DEBUG ServletHandler: filterNameMap={org.apache.spark.ui.HttpSecurityFilter-79656208=org.apache.spark.ui.HttpSecurityFilter-79656208==org.apache.spark.ui.HttpSecurityFilter@79656208{inst=false,async=true,src=EMBEDDED:null}}
21/07/22 08:16:02.888 Thread-3 DEBUG ServletHandler: pathFilters=[[/*]/[]/[ERROR, ASYNC, INCLUDE, FORWARD, REQUEST]=>org.apache.spark.ui.HttpSecurityFilter-79656208]
21/07/22 08:16:02.888 Thread-3 DEBUG ServletHandler: servletFilterMap={}
21/07/22 08:16:02.888 Thread-3 DEBUG ServletHandler: servletPathMap=PathMappings[size=1]
21/07/22 08:16:02.889 Thread-3 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-2dfa736d=org.apache.spark.ui.JettyUtils$$anon$1-2dfa736d==org.apache.spark.ui.JettyUtils$$anon$1@3fc432a5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
21/07/22 08:16:02.889 Thread-3 DEBUG AbstractHandler: starting ServletHandler@4082308d{STARTING}
21/07/22 08:16:02.889 Thread-3 DEBUG AbstractLifeCycle: STARTED @125380ms ServletHandler@4082308d{STARTED}
21/07/22 08:16:02.890 Thread-3 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.HttpSecurityFilter-79656208==org.apache.spark.ui.HttpSecurityFilter@79656208{inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.890 Thread-3 DEBUG AbstractLifeCycle: STARTED @125381ms org.apache.spark.ui.HttpSecurityFilter-79656208==org.apache.spark.ui.HttpSecurityFilter@79656208{inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.890 Thread-3 DEBUG FilterHolder: Filter.init org.apache.spark.ui.HttpSecurityFilter@64a51ac7
21/07/22 08:16:02.891 Thread-3 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$1-2dfa736d==org.apache.spark.ui.JettyUtils$$anon$1@3fc432a5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.891 Thread-3 DEBUG AbstractLifeCycle: STARTED @125382ms org.apache.spark.ui.JettyUtils$$anon$1-2dfa736d==org.apache.spark.ui.JettyUtils$$anon$1@3fc432a5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.891 Thread-3 DEBUG ServletHolder: Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$1-2dfa736d
21/07/22 08:16:02.892 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6e51a165{/StreamingQuery/json,null,AVAILABLE,@Spark}
21/07/22 08:16:02.892 Thread-3 DEBUG AbstractLifeCycle: STARTED @125383ms o.s.j.s.ServletContextHandler@6e51a165{/StreamingQuery/json,null,AVAILABLE,@Spark}
21/07/22 08:16:02.892 Thread-3 DEBUG AbstractLifeCycle: starting GzipHandler@62bc4102{STOPPED,min=32,inflate=-1}
21/07/22 08:16:02.893 Thread-3 DEBUG ContainerLifeCycle: GzipHandler@62bc4102{STARTING,min=32,inflate=-1} added {DeflaterPool@36025179{STOPPED,size=0,capacity=UNLIMITED},AUTO}
21/07/22 08:16:02.893 Thread-3 DEBUG AbstractHandler: starting GzipHandler@62bc4102{STARTING,min=32,inflate=-1}
21/07/22 08:16:02.893 Thread-3 DEBUG AbstractLifeCycle: starting DeflaterPool@36025179{STOPPED,size=0,capacity=UNLIMITED}
21/07/22 08:16:02.893 Thread-3 DEBUG AbstractLifeCycle: STARTED @125384ms DeflaterPool@36025179{STARTED,size=0,capacity=UNLIMITED}
21/07/22 08:16:02.893 Thread-3 DEBUG AbstractLifeCycle: STARTED @125384ms GzipHandler@62bc4102{STARTED,min=32,inflate=-1}
21/07/22 08:16:02.893 Thread-3 DEBUG DecoratedObjectFactory: Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@f980c97
21/07/22 08:16:02.894 Thread-3 DEBUG ContainerLifeCycle: o.s.j.s.ServletContextHandler@43274c81{/,null,STOPPED} added {ServletHandler@678a5405{STOPPED},MANAGED}
21/07/22 08:16:02.894 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@678a5405{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$1-64281bfc==org.apache.spark.ui.JettyUtils$$anon$1@991e0200{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
21/07/22 08:16:02.894 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@678a5405{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$1-64281bfc,POJO}
21/07/22 08:16:02.895 Thread-3 DEBUG DecoratedObjectFactory: Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@2b9b5f5a
21/07/22 08:16:02.896 Thread-3 DEBUG ContainerLifeCycle: o.s.j.s.ServletContextHandler@52a15dec{/,null,STOPPED} added {ServletHandler@67be01c1{STOPPED},MANAGED}
21/07/22 08:16:02.897 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@67be01c1{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$1-436d0131==org.apache.spark.ui.JettyUtils$$anon$1@9db3a6fa{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
21/07/22 08:16:02.897 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@67be01c1{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$1-436d0131,POJO}
21/07/22 08:16:02.898 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@678a5405{STOPPED} added {org.apache.spark.ui.HttpSecurityFilter-32fde789==org.apache.spark.ui.HttpSecurityFilter@32fde789{inst=false,async=true,src=EMBEDDED:null},AUTO}
21/07/22 08:16:02.898 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@678a5405{STOPPED} added {[/*]/[]/[ERROR, ASYNC, INCLUDE, FORWARD, REQUEST]=>org.apache.spark.ui.HttpSecurityFilter-32fde789,POJO}
21/07/22 08:16:02.899 Thread-3 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
21/07/22 08:16:02.899 Thread-3 DEBUG GzipHandler: GzipHandler@6aeb8207{STOPPED,min=32,inflate=-1} mime types IncludeExclude@7dcc2692{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
21/07/22 08:16:02.900 Thread-3 DEBUG ContainerLifeCycle: GzipHandler@6aeb8207{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@43274c81{/StreamingQuery/statistics,null,STOPPED,@Spark},MANAGED}
21/07/22 08:16:02.900 Thread-3 DEBUG ContextHandlerCollection: ->[{GzipHandler@2defb1ab{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@701ab0d5{/,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.900 Thread-3 DEBUG ContextHandlerCollection: storage/rdd->[{GzipHandler@6706c0d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4d8043cd{/storage/rdd,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.901 Thread-3 DEBUG ContextHandlerCollection: storage->[{GzipHandler@26c88166{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5eff2d7c{/storage,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.901 Thread-3 DEBUG ContextHandlerCollection: storage/rdd/json->[{GzipHandler@7671e0ca{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5f39e389{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.901 Thread-3 DEBUG ContextHandlerCollection: SQL/execution/json->[{GzipHandler@26436546{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@57455f84{/SQL/execution/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.902 Thread-3 DEBUG ContextHandlerCollection: api->[{GzipHandler@23d67587{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@781f9b06{/api,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.902 Thread-3 DEBUG ContextHandlerCollection: stages/pool/json->[{GzipHandler@41f7132a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@78513f9f{/stages/pool/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.902 Thread-3 DEBUG ContextHandlerCollection: stages/pool->[{GzipHandler@3879dc6e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@cd005ca{/stages/pool,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.902 Thread-3 DEBUG ContextHandlerCollection: jobs/json->[{GzipHandler@52df772{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5b7b61c9{/jobs/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.903 Thread-3 DEBUG ContextHandlerCollection: static->[{GzipHandler@1547d01d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@215aef98{/static,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.903 Thread-3 DEBUG ContextHandlerCollection: executors/json->[{GzipHandler@6cd0efdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@777c6cfe{/executors/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.903 Thread-3 DEBUG ContextHandlerCollection: stages/stage/json->[{GzipHandler@1a3cd484{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fd4c468{/stages/stage/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.904 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery/json->[{GzipHandler@62bc4102{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e51a165{/StreamingQuery/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.904 Thread-3 DEBUG ContextHandlerCollection: executors/threadDump/json->[{GzipHandler@704026d6{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@68480188{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.904 Thread-3 DEBUG ContextHandlerCollection: environment/json->[{GzipHandler@5b2cc7fb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@90c99e0{/environment/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.904 Thread-3 DEBUG ContextHandlerCollection: jobs/job/json->[{GzipHandler@30696d4d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@458522e3{/jobs/job/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.904 Thread-3 DEBUG ContextHandlerCollection: jobs->[{GzipHandler@4c0d1730{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@18bd25d{/jobs,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.905 Thread-3 DEBUG ContextHandlerCollection: stages/json->[{GzipHandler@3d8afef1{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@60a618c9{/stages/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.905 Thread-3 DEBUG ContextHandlerCollection: stages/stage->[{GzipHandler@a4e5a4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2841b357{/stages/stage,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.905 Thread-3 DEBUG ContextHandlerCollection: storage/json->[{GzipHandler@7b5cf3f8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@54e5e988{/storage/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.906 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery->[{GzipHandler@33b27550{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@225c5550{/StreamingQuery,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.906 Thread-3 DEBUG ContextHandlerCollection: SQL->[{GzipHandler@5fccdf34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@55ccca34{/SQL,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.906 Thread-3 DEBUG ContextHandlerCollection: static/sql->[{GzipHandler@355bdf4a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@23d5b0f2{/static/sql,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.906 Thread-3 DEBUG ContextHandlerCollection: stages/stage/kill->[{GzipHandler@12758580{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@57c24406{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.906 Thread-3 DEBUG ContextHandlerCollection: jobs/job->[{GzipHandler@e2a7705{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6791ecc8{/jobs/job,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.907 Thread-3 DEBUG ContextHandlerCollection: environment->[{GzipHandler@37262c8b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6210854d{/environment,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.907 Thread-3 DEBUG ContextHandlerCollection: stages->[{GzipHandler@5cec14aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4f19d1f0{/stages,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.910 Thread-3 DEBUG ContextHandlerCollection: executors->[{GzipHandler@620bd332{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4b1d0e67{/executors,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.910 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery/statistics->[{GzipHandler@6aeb8207{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@43274c81{/StreamingQuery/statistics,null,STOPPED,@Spark}]}]
21/07/22 08:16:02.911 Thread-3 DEBUG ContextHandlerCollection: SQL/json->[{GzipHandler@3f441229{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@441f6bf9{/SQL/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.911 Thread-3 DEBUG ContextHandlerCollection: jobs/job/kill->[{GzipHandler@3980333e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4acaf3b1{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.912 Thread-3 DEBUG ContextHandlerCollection: metrics/json->[{GzipHandler@57539e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@18887d17{/metrics/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.912 Thread-3 DEBUG ContextHandlerCollection: SQL/execution->[{GzipHandler@39dbf83f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2a21d39f{/SQL/execution,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.912 Thread-3 DEBUG ContextHandlerCollection: executors/threadDump->[{GzipHandler@26916077{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77303614{/executors/threadDump,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.912 Thread-3 DEBUG ContainerLifeCycle: ContextHandlerCollection@1eb0be58{STARTED} added {GzipHandler@6aeb8207{STOPPED,min=32,inflate=-1},UNMANAGED}
21/07/22 08:16:02.913 Thread-3 DEBUG AbstractLifeCycle: starting o.s.j.s.ServletContextHandler@43274c81{/StreamingQuery/statistics,null,STOPPED,@Spark}
21/07/22 08:16:02.913 Thread-3 DEBUG AbstractHandler: starting o.s.j.s.ServletContextHandler@43274c81{/StreamingQuery/statistics,null,STARTING,@Spark}
21/07/22 08:16:02.913 Thread-3 DEBUG AbstractLifeCycle: starting ServletHandler@678a5405{STOPPED}
21/07/22 08:16:02.913 Thread-3 DEBUG ServletHandler: Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-64281bfc[EMBEDDED:null]
21/07/22 08:16:02.913 Thread-3 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$1-64281bfc==org.apache.spark.ui.JettyUtils$$anon$1@991e0200{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
21/07/22 08:16:02.913 Thread-3 DEBUG ServletHandler: filterNameMap={org.apache.spark.ui.HttpSecurityFilter-32fde789=org.apache.spark.ui.HttpSecurityFilter-32fde789==org.apache.spark.ui.HttpSecurityFilter@32fde789{inst=false,async=true,src=EMBEDDED:null}}
21/07/22 08:16:02.914 Thread-3 DEBUG ServletHandler: pathFilters=[[/*]/[]/[ERROR, ASYNC, INCLUDE, FORWARD, REQUEST]=>org.apache.spark.ui.HttpSecurityFilter-32fde789]
21/07/22 08:16:02.914 Thread-3 DEBUG ServletHandler: servletFilterMap={}
21/07/22 08:16:02.914 Thread-3 DEBUG ServletHandler: servletPathMap=PathMappings[size=1]
21/07/22 08:16:02.914 Thread-3 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-64281bfc=org.apache.spark.ui.JettyUtils$$anon$1-64281bfc==org.apache.spark.ui.JettyUtils$$anon$1@991e0200{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
21/07/22 08:16:02.914 Thread-3 DEBUG AbstractHandler: starting ServletHandler@678a5405{STARTING}
21/07/22 08:16:02.915 Thread-3 DEBUG AbstractLifeCycle: STARTED @125406ms ServletHandler@678a5405{STARTED}
21/07/22 08:16:02.915 Thread-3 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.HttpSecurityFilter-32fde789==org.apache.spark.ui.HttpSecurityFilter@32fde789{inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.915 Thread-3 DEBUG AbstractLifeCycle: STARTED @125406ms org.apache.spark.ui.HttpSecurityFilter-32fde789==org.apache.spark.ui.HttpSecurityFilter@32fde789{inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.916 Thread-3 DEBUG FilterHolder: Filter.init org.apache.spark.ui.HttpSecurityFilter@3d52e0bb
21/07/22 08:16:02.916 Thread-3 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$1-64281bfc==org.apache.spark.ui.JettyUtils$$anon$1@991e0200{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.916 Thread-3 DEBUG AbstractLifeCycle: STARTED @125407ms org.apache.spark.ui.JettyUtils$$anon$1-64281bfc==org.apache.spark.ui.JettyUtils$$anon$1@991e0200{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.916 Thread-3 DEBUG ServletHolder: Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$1-64281bfc
21/07/22 08:16:02.916 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@43274c81{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
21/07/22 08:16:02.916 Thread-3 DEBUG AbstractLifeCycle: STARTED @125407ms o.s.j.s.ServletContextHandler@43274c81{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
21/07/22 08:16:02.916 Thread-3 DEBUG AbstractLifeCycle: starting GzipHandler@6aeb8207{STOPPED,min=32,inflate=-1}
21/07/22 08:16:02.916 Thread-3 DEBUG ContainerLifeCycle: GzipHandler@6aeb8207{STARTING,min=32,inflate=-1} added {DeflaterPool@21b6b935{STOPPED,size=0,capacity=UNLIMITED},AUTO}
21/07/22 08:16:02.916 Thread-3 DEBUG AbstractHandler: starting GzipHandler@6aeb8207{STARTING,min=32,inflate=-1}
21/07/22 08:16:02.916 Thread-3 DEBUG AbstractLifeCycle: starting DeflaterPool@21b6b935{STOPPED,size=0,capacity=UNLIMITED}
21/07/22 08:16:02.917 Thread-3 DEBUG AbstractLifeCycle: STARTED @125408ms DeflaterPool@21b6b935{STARTED,size=0,capacity=UNLIMITED}
21/07/22 08:16:02.917 Thread-3 DEBUG AbstractLifeCycle: STARTED @125408ms GzipHandler@6aeb8207{STARTED,min=32,inflate=-1}
21/07/22 08:16:02.917 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@67be01c1{STOPPED} added {org.apache.spark.ui.HttpSecurityFilter-695956cc==org.apache.spark.ui.HttpSecurityFilter@695956cc{inst=false,async=true,src=EMBEDDED:null},AUTO}
21/07/22 08:16:02.917 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@67be01c1{STOPPED} added {[/*]/[]/[ERROR, ASYNC, INCLUDE, FORWARD, REQUEST]=>org.apache.spark.ui.HttpSecurityFilter-695956cc,POJO}
21/07/22 08:16:02.918 Thread-3 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
21/07/22 08:16:02.918 Thread-3 DEBUG GzipHandler: GzipHandler@1fb3df0a{STOPPED,min=32,inflate=-1} mime types IncludeExclude@35ceb37f{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
21/07/22 08:16:02.919 Thread-3 DEBUG ContainerLifeCycle: GzipHandler@1fb3df0a{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@52a15dec{/StreamingQuery/statistics/json,null,STOPPED,@Spark},MANAGED}
21/07/22 08:16:02.919 Thread-3 DEBUG ContextHandlerCollection: ->[{GzipHandler@2defb1ab{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@701ab0d5{/,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.920 Thread-3 DEBUG ContextHandlerCollection: storage/rdd->[{GzipHandler@6706c0d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4d8043cd{/storage/rdd,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.920 Thread-3 DEBUG ContextHandlerCollection: storage->[{GzipHandler@26c88166{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5eff2d7c{/storage,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.920 Thread-3 DEBUG ContextHandlerCollection: storage/rdd/json->[{GzipHandler@7671e0ca{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5f39e389{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.920 Thread-3 DEBUG ContextHandlerCollection: SQL/execution/json->[{GzipHandler@26436546{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@57455f84{/SQL/execution/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.921 Thread-3 DEBUG ContextHandlerCollection: api->[{GzipHandler@23d67587{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@781f9b06{/api,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.921 Thread-3 DEBUG ContextHandlerCollection: stages/pool/json->[{GzipHandler@41f7132a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@78513f9f{/stages/pool/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.921 Thread-3 DEBUG ContextHandlerCollection: stages/pool->[{GzipHandler@3879dc6e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@cd005ca{/stages/pool,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.921 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery/statistics/json->[{GzipHandler@1fb3df0a{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@52a15dec{/StreamingQuery/statistics/json,null,STOPPED,@Spark}]}]
21/07/22 08:16:02.922 Thread-3 DEBUG ContextHandlerCollection: jobs/json->[{GzipHandler@52df772{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5b7b61c9{/jobs/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.922 Thread-3 DEBUG ContextHandlerCollection: static->[{GzipHandler@1547d01d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@215aef98{/static,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.922 Thread-3 DEBUG ContextHandlerCollection: executors/json->[{GzipHandler@6cd0efdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@777c6cfe{/executors/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.922 Thread-3 DEBUG ContextHandlerCollection: stages/stage/json->[{GzipHandler@1a3cd484{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fd4c468{/stages/stage/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.923 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery/json->[{GzipHandler@62bc4102{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e51a165{/StreamingQuery/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.923 Thread-3 DEBUG ContextHandlerCollection: executors/threadDump/json->[{GzipHandler@704026d6{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@68480188{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.923 Thread-3 DEBUG ContextHandlerCollection: environment/json->[{GzipHandler@5b2cc7fb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@90c99e0{/environment/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.924 Thread-3 DEBUG ContextHandlerCollection: jobs/job/json->[{GzipHandler@30696d4d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@458522e3{/jobs/job/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.924 Thread-3 DEBUG ContextHandlerCollection: jobs->[{GzipHandler@4c0d1730{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@18bd25d{/jobs,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.925 Thread-3 DEBUG ContextHandlerCollection: stages/json->[{GzipHandler@3d8afef1{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@60a618c9{/stages/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.925 Thread-3 DEBUG ContextHandlerCollection: stages/stage->[{GzipHandler@a4e5a4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2841b357{/stages/stage,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.926 Thread-3 DEBUG ContextHandlerCollection: storage/json->[{GzipHandler@7b5cf3f8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@54e5e988{/storage/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.926 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery->[{GzipHandler@33b27550{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@225c5550{/StreamingQuery,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.926 Thread-3 DEBUG ContextHandlerCollection: SQL->[{GzipHandler@5fccdf34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@55ccca34{/SQL,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.926 Thread-3 DEBUG ContextHandlerCollection: static/sql->[{GzipHandler@355bdf4a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@23d5b0f2{/static/sql,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.927 Thread-3 DEBUG ContextHandlerCollection: stages/stage/kill->[{GzipHandler@12758580{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@57c24406{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.927 Thread-3 DEBUG ContextHandlerCollection: jobs/job->[{GzipHandler@e2a7705{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6791ecc8{/jobs/job,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.927 Thread-3 DEBUG ContextHandlerCollection: environment->[{GzipHandler@37262c8b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6210854d{/environment,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.928 Thread-3 DEBUG ContextHandlerCollection: stages->[{GzipHandler@5cec14aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4f19d1f0{/stages,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.928 Thread-3 DEBUG ContextHandlerCollection: executors->[{GzipHandler@620bd332{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4b1d0e67{/executors,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.928 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery/statistics->[{GzipHandler@6aeb8207{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@43274c81{/StreamingQuery/statistics,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.928 Thread-3 DEBUG ContextHandlerCollection: SQL/json->[{GzipHandler@3f441229{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@441f6bf9{/SQL/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.929 Thread-3 DEBUG ContextHandlerCollection: jobs/job/kill->[{GzipHandler@3980333e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4acaf3b1{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.930 Thread-3 DEBUG ContextHandlerCollection: metrics/json->[{GzipHandler@57539e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@18887d17{/metrics/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.930 Thread-3 DEBUG ContextHandlerCollection: SQL/execution->[{GzipHandler@39dbf83f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2a21d39f{/SQL/execution,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.931 Thread-3 DEBUG ContextHandlerCollection: executors/threadDump->[{GzipHandler@26916077{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77303614{/executors/threadDump,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.931 Thread-3 DEBUG ContainerLifeCycle: ContextHandlerCollection@1eb0be58{STARTED} added {GzipHandler@1fb3df0a{STOPPED,min=32,inflate=-1},UNMANAGED}
21/07/22 08:16:02.932 Thread-3 DEBUG AbstractLifeCycle: starting o.s.j.s.ServletContextHandler@52a15dec{/StreamingQuery/statistics/json,null,STOPPED,@Spark}
21/07/22 08:16:02.932 Thread-3 DEBUG AbstractHandler: starting o.s.j.s.ServletContextHandler@52a15dec{/StreamingQuery/statistics/json,null,STARTING,@Spark}
21/07/22 08:16:02.932 Thread-3 DEBUG AbstractLifeCycle: starting ServletHandler@67be01c1{STOPPED}
21/07/22 08:16:02.933 Thread-3 DEBUG ServletHandler: Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-436d0131[EMBEDDED:null]
21/07/22 08:16:02.935 Thread-3 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$1-436d0131==org.apache.spark.ui.JettyUtils$$anon$1@9db3a6fa{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
21/07/22 08:16:02.935 Thread-3 DEBUG ServletHandler: filterNameMap={org.apache.spark.ui.HttpSecurityFilter-695956cc=org.apache.spark.ui.HttpSecurityFilter-695956cc==org.apache.spark.ui.HttpSecurityFilter@695956cc{inst=false,async=true,src=EMBEDDED:null}}
21/07/22 08:16:02.936 Thread-3 DEBUG ServletHandler: pathFilters=[[/*]/[]/[ERROR, ASYNC, INCLUDE, FORWARD, REQUEST]=>org.apache.spark.ui.HttpSecurityFilter-695956cc]
21/07/22 08:16:02.936 Thread-3 DEBUG ServletHandler: servletFilterMap={}
21/07/22 08:16:02.936 Thread-3 DEBUG ServletHandler: servletPathMap=PathMappings[size=1]
21/07/22 08:16:02.942 Thread-3 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-436d0131=org.apache.spark.ui.JettyUtils$$anon$1-436d0131==org.apache.spark.ui.JettyUtils$$anon$1@9db3a6fa{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
21/07/22 08:16:02.943 Thread-3 DEBUG AbstractHandler: starting ServletHandler@67be01c1{STARTING}
21/07/22 08:16:02.943 Thread-3 DEBUG AbstractLifeCycle: STARTED @125434ms ServletHandler@67be01c1{STARTED}
21/07/22 08:16:02.943 Thread-3 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.HttpSecurityFilter-695956cc==org.apache.spark.ui.HttpSecurityFilter@695956cc{inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.943 Thread-3 DEBUG AbstractLifeCycle: STARTED @125434ms org.apache.spark.ui.HttpSecurityFilter-695956cc==org.apache.spark.ui.HttpSecurityFilter@695956cc{inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.943 Thread-3 DEBUG FilterHolder: Filter.init org.apache.spark.ui.HttpSecurityFilter@21a5dbe2
21/07/22 08:16:02.943 Thread-3 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$1-436d0131==org.apache.spark.ui.JettyUtils$$anon$1@9db3a6fa{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.944 Thread-3 DEBUG AbstractLifeCycle: STARTED @125435ms org.apache.spark.ui.JettyUtils$$anon$1-436d0131==org.apache.spark.ui.JettyUtils$$anon$1@9db3a6fa{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:02.944 Thread-3 DEBUG ServletHolder: Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$1-436d0131
21/07/22 08:16:02.944 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@52a15dec{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
21/07/22 08:16:02.944 Thread-3 DEBUG AbstractLifeCycle: STARTED @125435ms o.s.j.s.ServletContextHandler@52a15dec{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
21/07/22 08:16:02.944 Thread-3 DEBUG AbstractLifeCycle: starting GzipHandler@1fb3df0a{STOPPED,min=32,inflate=-1}
21/07/22 08:16:02.944 Thread-3 DEBUG ContainerLifeCycle: GzipHandler@1fb3df0a{STARTING,min=32,inflate=-1} added {DeflaterPool@269062c{STOPPED,size=0,capacity=UNLIMITED},AUTO}
21/07/22 08:16:02.944 Thread-3 DEBUG AbstractHandler: starting GzipHandler@1fb3df0a{STARTING,min=32,inflate=-1}
21/07/22 08:16:02.945 Thread-3 DEBUG AbstractLifeCycle: starting DeflaterPool@269062c{STOPPED,size=0,capacity=UNLIMITED}
21/07/22 08:16:02.945 Thread-3 DEBUG AbstractLifeCycle: STARTED @125436ms DeflaterPool@269062c{STARTED,size=0,capacity=UNLIMITED}
21/07/22 08:16:02.947 Thread-3 DEBUG AbstractLifeCycle: STARTED @125436ms GzipHandler@1fb3df0a{STARTED,min=32,inflate=-1}
21/07/22 08:16:02.948 Thread-3 DEBUG DecoratedObjectFactory: Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@1bf804f5
21/07/22 08:16:02.951 Thread-3 DEBUG ContainerLifeCycle: o.s.j.s.ServletContextHandler@6e5e38c2{/,null,STOPPED} added {ServletHandler@3965644f{STOPPED},MANAGED}
21/07/22 08:16:02.952 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@3965644f{STOPPED} added {org.sparkproject.jetty.servlet.DefaultServlet-35a9b940==org.sparkproject.jetty.servlet.DefaultServlet@228d2135{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
21/07/22 08:16:02.957 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@3965644f{STOPPED} added {[/]=>org.sparkproject.jetty.servlet.DefaultServlet-35a9b940,POJO}
21/07/22 08:16:02.962 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@3965644f{STOPPED} added {org.apache.spark.ui.HttpSecurityFilter-6eaddcac==org.apache.spark.ui.HttpSecurityFilter@6eaddcac{inst=false,async=true,src=EMBEDDED:null},AUTO}
21/07/22 08:16:02.962 Thread-3 DEBUG ContainerLifeCycle: ServletHandler@3965644f{STOPPED} added {[/*]/[]/[ERROR, ASYNC, INCLUDE, FORWARD, REQUEST]=>org.apache.spark.ui.HttpSecurityFilter-6eaddcac,POJO}
21/07/22 08:16:02.963 Thread-3 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
21/07/22 08:16:02.963 Thread-3 DEBUG GzipHandler: GzipHandler@71aa6193{STOPPED,min=32,inflate=-1} mime types IncludeExclude@637f24f5{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
21/07/22 08:16:02.964 Thread-3 DEBUG ContainerLifeCycle: GzipHandler@71aa6193{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@6e5e38c2{/static/sql,null,STOPPED,@Spark},MANAGED}
21/07/22 08:16:02.964 Thread-3 DEBUG ContextHandlerCollection: ->[{GzipHandler@2defb1ab{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@701ab0d5{/,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.964 Thread-3 DEBUG ContextHandlerCollection: storage/rdd->[{GzipHandler@6706c0d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4d8043cd{/storage/rdd,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.965 Thread-3 DEBUG ContextHandlerCollection: storage->[{GzipHandler@26c88166{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5eff2d7c{/storage,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.967 Thread-3 DEBUG ContextHandlerCollection: storage/rdd/json->[{GzipHandler@7671e0ca{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5f39e389{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.967 Thread-3 DEBUG ContextHandlerCollection: SQL/execution/json->[{GzipHandler@26436546{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@57455f84{/SQL/execution/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.968 Thread-3 DEBUG ContextHandlerCollection: api->[{GzipHandler@23d67587{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@781f9b06{/api,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.968 Thread-3 DEBUG ContextHandlerCollection: stages/pool/json->[{GzipHandler@41f7132a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@78513f9f{/stages/pool/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.969 Thread-3 DEBUG ContextHandlerCollection: stages/pool->[{GzipHandler@3879dc6e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@cd005ca{/stages/pool,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.977 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery/statistics/json->[{GzipHandler@1fb3df0a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@52a15dec{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.977 Thread-3 DEBUG ContextHandlerCollection: jobs/json->[{GzipHandler@52df772{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5b7b61c9{/jobs/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.978 Thread-3 DEBUG ContextHandlerCollection: static->[{GzipHandler@1547d01d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@215aef98{/static,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.979 Thread-3 DEBUG ContextHandlerCollection: executors/json->[{GzipHandler@6cd0efdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@777c6cfe{/executors/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.981 Thread-3 DEBUG ContextHandlerCollection: stages/stage/json->[{GzipHandler@1a3cd484{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fd4c468{/stages/stage/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.982 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery/json->[{GzipHandler@62bc4102{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e51a165{/StreamingQuery/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.982 Thread-3 DEBUG ContextHandlerCollection: executors/threadDump/json->[{GzipHandler@704026d6{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@68480188{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.982 Thread-3 DEBUG ContextHandlerCollection: environment/json->[{GzipHandler@5b2cc7fb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@90c99e0{/environment/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.983 Thread-3 DEBUG ContextHandlerCollection: jobs/job/json->[{GzipHandler@30696d4d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@458522e3{/jobs/job/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.983 Thread-3 DEBUG ContextHandlerCollection: jobs->[{GzipHandler@4c0d1730{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@18bd25d{/jobs,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.983 Thread-3 DEBUG ContextHandlerCollection: stages/json->[{GzipHandler@3d8afef1{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@60a618c9{/stages/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.983 Thread-3 DEBUG ContextHandlerCollection: stages/stage->[{GzipHandler@a4e5a4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2841b357{/stages/stage,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.984 Thread-3 DEBUG ContextHandlerCollection: storage/json->[{GzipHandler@7b5cf3f8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@54e5e988{/storage/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.984 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery->[{GzipHandler@33b27550{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@225c5550{/StreamingQuery,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:02.999 Thread-3 DEBUG ContextHandlerCollection: SQL->[{GzipHandler@5fccdf34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@55ccca34{/SQL,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:03.000 Thread-3 DEBUG ContextHandlerCollection: static/sql->[{GzipHandler@355bdf4a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@23d5b0f2{/static/sql,null,AVAILABLE,@Spark}]}, {GzipHandler@71aa6193{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e5e38c2{/static/sql,null,STOPPED,@Spark}]}]
21/07/22 08:16:03.000 Thread-3 DEBUG ContextHandlerCollection: stages/stage/kill->[{GzipHandler@12758580{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@57c24406{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:03.001 Thread-3 DEBUG ContextHandlerCollection: jobs/job->[{GzipHandler@e2a7705{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6791ecc8{/jobs/job,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:03.001 Thread-3 DEBUG ContextHandlerCollection: environment->[{GzipHandler@37262c8b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6210854d{/environment,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:03.001 Thread-3 DEBUG ContextHandlerCollection: stages->[{GzipHandler@5cec14aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4f19d1f0{/stages,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:03.001 Thread-3 DEBUG ContextHandlerCollection: executors->[{GzipHandler@620bd332{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4b1d0e67{/executors,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:03.002 Thread-3 DEBUG ContextHandlerCollection: StreamingQuery/statistics->[{GzipHandler@6aeb8207{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@43274c81{/StreamingQuery/statistics,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:03.002 Thread-3 DEBUG ContextHandlerCollection: SQL/json->[{GzipHandler@3f441229{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@441f6bf9{/SQL/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:03.002 Thread-3 DEBUG ContextHandlerCollection: jobs/job/kill->[{GzipHandler@3980333e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4acaf3b1{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:03.006 Thread-3 DEBUG ContextHandlerCollection: metrics/json->[{GzipHandler@57539e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@18887d17{/metrics/json,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:03.006 Thread-3 DEBUG ContextHandlerCollection: SQL/execution->[{GzipHandler@39dbf83f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2a21d39f{/SQL/execution,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:03.006 Thread-3 DEBUG ContextHandlerCollection: executors/threadDump->[{GzipHandler@26916077{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77303614{/executors/threadDump,null,AVAILABLE,@Spark}]}]
21/07/22 08:16:03.007 Thread-3 DEBUG ContainerLifeCycle: ContextHandlerCollection@1eb0be58{STARTED} added {GzipHandler@71aa6193{STOPPED,min=32,inflate=-1},UNMANAGED}
21/07/22 08:16:03.007 Thread-3 DEBUG AbstractLifeCycle: starting o.s.j.s.ServletContextHandler@6e5e38c2{/static/sql,null,STOPPED,@Spark}
21/07/22 08:16:03.007 Thread-3 DEBUG AbstractHandler: starting o.s.j.s.ServletContextHandler@6e5e38c2{/static/sql,null,STARTING,@Spark}
21/07/22 08:16:03.007 Thread-3 DEBUG AbstractLifeCycle: starting ServletHandler@3965644f{STOPPED}
21/07/22 08:16:03.008 Thread-3 DEBUG ServletHandler: Path=/[EMBEDDED:null] mapped to servlet=org.sparkproject.jetty.servlet.DefaultServlet-35a9b940[EMBEDDED:null]
21/07/22 08:16:03.008 Thread-3 DEBUG PathMappings: Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.sparkproject.jetty.servlet.DefaultServlet-35a9b940==org.sparkproject.jetty.servlet.DefaultServlet@228d2135{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
21/07/22 08:16:03.008 Thread-3 DEBUG ServletHandler: filterNameMap={org.apache.spark.ui.HttpSecurityFilter-6eaddcac=org.apache.spark.ui.HttpSecurityFilter-6eaddcac==org.apache.spark.ui.HttpSecurityFilter@6eaddcac{inst=false,async=true,src=EMBEDDED:null}}
21/07/22 08:16:03.008 Thread-3 DEBUG ServletHandler: pathFilters=[[/*]/[]/[ERROR, ASYNC, INCLUDE, FORWARD, REQUEST]=>org.apache.spark.ui.HttpSecurityFilter-6eaddcac]
21/07/22 08:16:03.008 Thread-3 DEBUG ServletHandler: servletFilterMap={}
21/07/22 08:16:03.008 Thread-3 DEBUG ServletHandler: servletPathMap=PathMappings[size=1]
21/07/22 08:16:03.008 Thread-3 DEBUG ServletHandler: servletNameMap={org.sparkproject.jetty.servlet.DefaultServlet-35a9b940=org.sparkproject.jetty.servlet.DefaultServlet-35a9b940==org.sparkproject.jetty.servlet.DefaultServlet@228d2135{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
21/07/22 08:16:03.008 Thread-3 DEBUG AbstractHandler: starting ServletHandler@3965644f{STARTING}
21/07/22 08:16:03.009 Thread-3 DEBUG AbstractLifeCycle: STARTED @125500ms ServletHandler@3965644f{STARTED}
21/07/22 08:16:03.009 Thread-3 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.HttpSecurityFilter-6eaddcac==org.apache.spark.ui.HttpSecurityFilter@6eaddcac{inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:03.009 Thread-3 DEBUG AbstractLifeCycle: STARTED @125500ms org.apache.spark.ui.HttpSecurityFilter-6eaddcac==org.apache.spark.ui.HttpSecurityFilter@6eaddcac{inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:03.009 Thread-3 DEBUG FilterHolder: Filter.init org.apache.spark.ui.HttpSecurityFilter@1be4f263
21/07/22 08:16:03.009 Thread-3 DEBUG AbstractLifeCycle: starting org.sparkproject.jetty.servlet.DefaultServlet-35a9b940==org.sparkproject.jetty.servlet.DefaultServlet@228d2135{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:03.009 Thread-3 DEBUG AbstractLifeCycle: STARTED @125500ms org.sparkproject.jetty.servlet.DefaultServlet-35a9b940==org.sparkproject.jetty.servlet.DefaultServlet@228d2135{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
21/07/22 08:16:03.009 Thread-3 DEBUG ServletHolder: Servlet.init null for org.sparkproject.jetty.servlet.DefaultServlet-35a9b940
21/07/22 08:16:03.010 Thread-3 DEBUG DefaultServlet: resource base = jar:file:/home/daniel/.local/lib/python3.8/site-packages/pyspark/jars/spark-sql_2.12-3.1.2.jar!/org/apache/spark/sql/execution/ui/static
21/07/22 08:16:03.010 Thread-3 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6e5e38c2{/static/sql,null,AVAILABLE,@Spark}
21/07/22 08:16:03.010 Thread-3 DEBUG AbstractLifeCycle: STARTED @125501ms o.s.j.s.ServletContextHandler@6e5e38c2{/static/sql,null,AVAILABLE,@Spark}
21/07/22 08:16:03.010 Thread-3 DEBUG AbstractLifeCycle: starting GzipHandler@71aa6193{STOPPED,min=32,inflate=-1}
21/07/22 08:16:03.010 Thread-3 DEBUG ContainerLifeCycle: GzipHandler@71aa6193{STARTING,min=32,inflate=-1} added {DeflaterPool@1bac150f{STOPPED,size=0,capacity=UNLIMITED},AUTO}
21/07/22 08:16:03.010 Thread-3 DEBUG AbstractHandler: starting GzipHandler@71aa6193{STARTING,min=32,inflate=-1}
21/07/22 08:16:03.010 Thread-3 DEBUG AbstractLifeCycle: starting DeflaterPool@1bac150f{STOPPED,size=0,capacity=UNLIMITED}
21/07/22 08:16:03.010 Thread-3 DEBUG AbstractLifeCycle: STARTED @125501ms DeflaterPool@1bac150f{STARTED,size=0,capacity=UNLIMITED}
21/07/22 08:16:03.010 Thread-3 DEBUG AbstractLifeCycle: STARTED @125501ms GzipHandler@71aa6193{STARTED,min=32,inflate=-1}
21/07/22 08:16:03.020 Thread-3 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6ed24962-8237-4a57-8db0-d0b3461980ef. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
21/07/22 08:16:03.098 Thread-3 DEBUG Tracer: sampler.classes = ; loaded no samplers
21/07/22 08:16:03.099 Thread-3 TRACE TracerId: ProcessID(fmt=%{tname}/%{ip}): computed process ID of "FSClient/172.17.0.1"
21/07/22 08:16:03.100 Thread-3 TRACE TracerPool: TracerPool(Global): adding tracer Tracer(FSClient/172.17.0.1)
21/07/22 08:16:03.100 Thread-3 DEBUG Tracer: span.receiver.classes = ; loaded no span receivers
21/07/22 08:16:03.100 Thread-3 TRACE Tracer: Created Tracer(FSClient/172.17.0.1) for FSClient
21/07/22 08:16:03.102 Thread-3 DEBUG FileSystem: Looking for FS supporting file
21/07/22 08:16:03.102 Thread-3 DEBUG FileSystem: looking for configuration option fs.file.impl
21/07/22 08:16:03.102 Thread-3 DEBUG FileSystem: Looking in service filesystems for implementation class
21/07/22 08:16:03.102 Thread-3 DEBUG FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
21/07/22 08:16:03.130 Thread-3 INFO MicroBatchExecution: Checkpoint root /tmp/temporary-6ed24962-8237-4a57-8db0-d0b3461980ef resolved to file:/tmp/temporary-6ed24962-8237-4a57-8db0-d0b3461980ef.
21/07/22 08:16:03.154 Thread-3 DEBUG UserGroupInformation: PrivilegedAction as:daniel (auth:SIMPLE) from:org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:333)
21/07/22 08:16:03.221 Thread-3 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6ed24962-8237-4a57-8db0-d0b3461980ef/metadata using temp file file:/tmp/temporary-6ed24962-8237-4a57-8db0-d0b3461980ef/.metadata.f2e93607-8fce-472b-8238-da120e987f6a.tmp
21/07/22 08:16:03.422 Thread-3 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6ed24962-8237-4a57-8db0-d0b3461980ef/.metadata.f2e93607-8fce-472b-8238-da120e987f6a.tmp to file:/tmp/temporary-6ed24962-8237-4a57-8db0-d0b3461980ef/metadata
21/07/22 08:16:03.473 Thread-3 DEBUG UserGroupInformation: PrivilegedAction as:daniel (auth:SIMPLE) from:org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:333)
21/07/22 08:16:03.507 Thread-3 DEBUG UserGroupInformation: PrivilegedAction as:daniel (auth:SIMPLE) from:org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:333)
21/07/22 08:16:03.551 Thread-3 INFO MicroBatchExecution: Starting [id = f7c4075d-830a-47c7-b2d6-32f9a13b390a, runId = bb98c755-b061-488e-b165-02dbc9e670a3]. Use file:/tmp/temporary-6ed24962-8237-4a57-8db0-d0b3461980ef to store the query checkpoint.
21/07/22 08:16:03.627 stream execution thread for [id = f7c4075d-830a-47c7-b2d6-32f9a13b390a, runId = bb98c755-b061-488e-b165-02dbc9e670a3] INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54699e05] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@4d2961dd]
21/07/22 08:16:03.655 stream execution thread for [id = f7c4075d-830a-47c7-b2d6-32f9a13b390a, runId = bb98c755-b061-488e-b165-02dbc9e670a3] ERROR MicroBatchExecution: Query [id = f7c4075d-830a-47c7-b2d6-32f9a13b390a, runId = bb98c755-b061-488e-b165-02dbc9e670a3] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.kafka010.KafkaConfigUpdater
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	... 33 more
21/07/22 08:16:03.701 dispatcher-event-loop-2 DEBUG StateStoreCoordinator: Deactivating instances related to checkpoint location bb98c755-b061-488e-b165-02dbc9e670a3: 
21/07/22 08:16:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:17:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:17:57.928 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Substitution after 1 iterations.
21/07/22 08:17:57.928 Thread-3 TRACE PlanChangeLogger: Batch Substitution has no effect.
21/07/22 08:17:57.928 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Disable Hints after 1 iterations.
21/07/22 08:17:57.928 Thread-3 TRACE PlanChangeLogger: Batch Disable Hints has no effect.
21/07/22 08:17:57.929 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Hints after 1 iterations.
21/07/22 08:17:57.929 Thread-3 TRACE PlanChangeLogger: Batch Hints has no effect.
21/07/22 08:17:57.929 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Simple Sanity Check after 1 iterations.
21/07/22 08:17:57.929 Thread-3 TRACE PlanChangeLogger: Batch Simple Sanity Check has no effect.
21/07/22 08:17:57.934 Thread-3 TRACE Analyzer$ResolveReferences: Attempting to resolve StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]
21/07/22 08:17:57.937 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Resolution after 1 iterations.
21/07/22 08:17:57.937 Thread-3 TRACE PlanChangeLogger: Batch Resolution has no effect.
21/07/22 08:17:57.937 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Apply Char Padding after 1 iterations.
21/07/22 08:17:57.937 Thread-3 TRACE PlanChangeLogger: Batch Apply Char Padding has no effect.
21/07/22 08:17:57.937 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Post-Hoc Resolution after 1 iterations.
21/07/22 08:17:57.938 Thread-3 TRACE PlanChangeLogger: Batch Post-Hoc Resolution has no effect.
21/07/22 08:17:57.938 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Normalize Alter Table after 1 iterations.
21/07/22 08:17:57.938 Thread-3 TRACE PlanChangeLogger: Batch Normalize Alter Table has no effect.
21/07/22 08:17:57.938 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove Unresolved Hints after 1 iterations.
21/07/22 08:17:57.938 Thread-3 TRACE PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
21/07/22 08:17:57.938 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Nondeterministic after 1 iterations.
21/07/22 08:17:57.938 Thread-3 TRACE PlanChangeLogger: Batch Nondeterministic has no effect.
21/07/22 08:17:57.938 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UDF after 1 iterations.
21/07/22 08:17:57.938 Thread-3 TRACE PlanChangeLogger: Batch UDF has no effect.
21/07/22 08:17:57.938 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UpdateNullability after 1 iterations.
21/07/22 08:17:57.938 Thread-3 TRACE PlanChangeLogger: Batch UpdateNullability has no effect.
21/07/22 08:17:57.938 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Subquery after 1 iterations.
21/07/22 08:17:57.939 Thread-3 TRACE PlanChangeLogger: Batch Subquery has no effect.
21/07/22 08:17:57.939 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Cleanup after 1 iterations.
21/07/22 08:17:57.939 Thread-3 TRACE PlanChangeLogger: Batch Cleanup has no effect.
21/07/22 08:17:57.939 Thread-3 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 86
Total time: 0.00878461 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/07/22 08:18:05.880 Thread-3 DEBUG SparkSqlParser: Parsing command: CAST(key AS STRING)
21/07/22 08:18:05.882 Thread-3 DEBUG SparkSqlParser: Parsing command: CAST(value AS STRING)
21/07/22 08:18:05.887 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Substitution after 1 iterations.
21/07/22 08:18:05.887 Thread-3 TRACE PlanChangeLogger: Batch Substitution has no effect.
21/07/22 08:18:05.888 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Disable Hints after 1 iterations.
21/07/22 08:18:05.888 Thread-3 TRACE PlanChangeLogger: Batch Disable Hints has no effect.
21/07/22 08:18:05.888 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Hints after 1 iterations.
21/07/22 08:18:05.888 Thread-3 TRACE PlanChangeLogger: Batch Hints has no effect.
21/07/22 08:18:05.888 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Simple Sanity Check after 1 iterations.
21/07/22 08:18:05.888 Thread-3 TRACE PlanChangeLogger: Batch Simple Sanity Check has no effect.
21/07/22 08:18:05.889 Thread-3 TRACE Analyzer$ResolveReferences: Attempting to resolve 'Project [unresolvedalias(cast('key as string), None), unresolvedalias(cast('value as string), None)]
21/07/22 08:18:05.891 Thread-3 DEBUG Analyzer$ResolveReferences: Resolving 'key to key#38
21/07/22 08:18:05.891 Thread-3 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#39
21/07/22 08:18:05.903 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences ===
!'Project [unresolvedalias(cast('key as string), None), unresolvedalias(cast('value as string), None)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          'Project [unresolvedalias(cast(key#38 as string), None), unresolvedalias(cast(value#39 as string), None)]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]
           
21/07/22 08:18:05.919 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases ===
!'Project [unresolvedalias(cast(key#38 as string), None), unresolvedalias(cast(value#39 as string), None)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Project [cast(key#38 as string) AS key#52, cast(value#39 as string) AS value#53]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]
           
21/07/22 08:18:05.934 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.ResolveTimeZone ===
 Project [cast(key#38 as string) AS key#52, cast(value#39 as string) AS value#53]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Project [cast(key#38 as string) AS key#52, cast(value#39 as string) AS value#53]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]
           
21/07/22 08:18:05.943 Thread-3 TRACE Analyzer$ResolveReferences: Attempting to resolve Project [cast(key#38 as string) AS key#52, cast(value#39 as string) AS value#53]
21/07/22 08:18:05.947 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Resolution after 2 iterations.
21/07/22 08:18:05.951 Thread-3 TRACE PlanChangeLogger: 
=== Result of Batch Resolution ===
!'Project [unresolvedalias(cast('key as string), None), unresolvedalias(cast('value as string), None)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Project [cast(key#38 as string) AS key#52, cast(value#39 as string) AS value#53]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]
          
21/07/22 08:18:05.952 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Apply Char Padding after 1 iterations.
21/07/22 08:18:05.952 Thread-3 TRACE PlanChangeLogger: Batch Apply Char Padding has no effect.
21/07/22 08:18:05.952 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Post-Hoc Resolution after 1 iterations.
21/07/22 08:18:05.952 Thread-3 TRACE PlanChangeLogger: Batch Post-Hoc Resolution has no effect.
21/07/22 08:18:05.953 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Normalize Alter Table after 1 iterations.
21/07/22 08:18:05.953 Thread-3 TRACE PlanChangeLogger: Batch Normalize Alter Table has no effect.
21/07/22 08:18:05.953 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove Unresolved Hints after 1 iterations.
21/07/22 08:18:05.953 Thread-3 TRACE PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
21/07/22 08:18:05.953 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Nondeterministic after 1 iterations.
21/07/22 08:18:05.953 Thread-3 TRACE PlanChangeLogger: Batch Nondeterministic has no effect.
21/07/22 08:18:05.953 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UDF after 1 iterations.
21/07/22 08:18:05.953 Thread-3 TRACE PlanChangeLogger: Batch UDF has no effect.
21/07/22 08:18:05.954 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UpdateNullability after 1 iterations.
21/07/22 08:18:05.954 Thread-3 TRACE PlanChangeLogger: Batch UpdateNullability has no effect.
21/07/22 08:18:05.954 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Subquery after 1 iterations.
21/07/22 08:18:05.954 Thread-3 TRACE PlanChangeLogger: Batch Subquery has no effect.
21/07/22 08:18:05.958 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.CleanupAliases ===
 Project [cast(key#38 as string) AS key#52, cast(value#39 as string) AS value#53]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Project [cast(key#38 as string) AS key#52, cast(value#39 as string) AS value#53]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]
           
21/07/22 08:18:05.958 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Cleanup after 2 iterations.
21/07/22 08:18:05.963 Thread-3 TRACE PlanChangeLogger: 
=== Result of Batch Cleanup ===
 Project [cast(key#38 as string) AS key#52, cast(value#39 as string) AS value#53]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Project [cast(key#38 as string) AS key#52, cast(value#39 as string) AS value#53]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]
          
21/07/22 08:18:05.963 Thread-3 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 150
Total time: 0.031503492 seconds
Total number of effective runs: 4
Total time of effective runs: 0.010407295 seconds
      
21/07/22 08:18:05.982 Thread-3 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b3b46562-618f-4f3f-b4c1-dac1a74afbb3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
21/07/22 08:18:05.990 Thread-3 INFO MicroBatchExecution: Checkpoint root /tmp/temporary-b3b46562-618f-4f3f-b4c1-dac1a74afbb3 resolved to file:/tmp/temporary-b3b46562-618f-4f3f-b4c1-dac1a74afbb3.
21/07/22 08:18:05.992 Thread-3 DEBUG UserGroupInformation: PrivilegedAction as:daniel (auth:SIMPLE) from:org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:333)
21/07/22 08:18:06.016 Thread-3 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b3b46562-618f-4f3f-b4c1-dac1a74afbb3/metadata using temp file file:/tmp/temporary-b3b46562-618f-4f3f-b4c1-dac1a74afbb3/.metadata.b632b616-ac15-47f1-b082-d96041717c10.tmp
21/07/22 08:18:06.035 Thread-3 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b3b46562-618f-4f3f-b4c1-dac1a74afbb3/.metadata.b632b616-ac15-47f1-b082-d96041717c10.tmp to file:/tmp/temporary-b3b46562-618f-4f3f-b4c1-dac1a74afbb3/metadata
21/07/22 08:18:06.042 Thread-3 DEBUG UserGroupInformation: PrivilegedAction as:daniel (auth:SIMPLE) from:org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:333)
21/07/22 08:18:06.051 Thread-3 DEBUG UserGroupInformation: PrivilegedAction as:daniel (auth:SIMPLE) from:org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:333)
21/07/22 08:18:06.059 Thread-3 INFO MicroBatchExecution: Starting [id = 8aff37aa-c65a-4fb9-a6eb-f45ee6c4bc36, runId = c3423808-8e38-43e7-9b05-f2ad16fa8856]. Use file:/tmp/temporary-b3b46562-618f-4f3f-b4c1-dac1a74afbb3 to store the query checkpoint.
21/07/22 08:18:06.064 stream execution thread for [id = 8aff37aa-c65a-4fb9-a6eb-f45ee6c4bc36, runId = c3423808-8e38-43e7-9b05-f2ad16fa8856] INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132]
21/07/22 08:18:06.067 stream execution thread for [id = 8aff37aa-c65a-4fb9-a6eb-f45ee6c4bc36, runId = c3423808-8e38-43e7-9b05-f2ad16fa8856] ERROR MicroBatchExecution: Query [id = 8aff37aa-c65a-4fb9-a6eb-f45ee6c4bc36, runId = c3423808-8e38-43e7-9b05-f2ad16fa8856] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/22 08:18:06.070 dispatcher-event-loop-1 DEBUG StateStoreCoordinator: Deactivating instances related to checkpoint location c3423808-8e38-43e7-9b05-f2ad16fa8856: 
21/07/22 08:18:17.754 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:18:47.672 Thread-3 DEBUG SparkSqlParser: Parsing command: CAST(key AS STRING)
21/07/22 08:18:47.674 Thread-3 DEBUG SparkSqlParser: Parsing command: CAST(value AS STRING)
21/07/22 08:18:47.680 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Substitution after 1 iterations.
21/07/22 08:18:47.681 Thread-3 TRACE PlanChangeLogger: Batch Substitution has no effect.
21/07/22 08:18:47.681 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Disable Hints after 1 iterations.
21/07/22 08:18:47.681 Thread-3 TRACE PlanChangeLogger: Batch Disable Hints has no effect.
21/07/22 08:18:47.682 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Hints after 1 iterations.
21/07/22 08:18:47.682 Thread-3 TRACE PlanChangeLogger: Batch Hints has no effect.
21/07/22 08:18:47.682 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Simple Sanity Check after 1 iterations.
21/07/22 08:18:47.682 Thread-3 TRACE PlanChangeLogger: Batch Simple Sanity Check has no effect.
21/07/22 08:18:47.684 Thread-3 TRACE Analyzer$ResolveReferences: Attempting to resolve 'Project [unresolvedalias(cast('key as string), None), unresolvedalias(cast('value as string), None)]
21/07/22 08:18:47.687 Thread-3 DEBUG Analyzer$ResolveReferences: Resolving 'key to key#38
21/07/22 08:18:47.690 Thread-3 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#39
21/07/22 08:18:47.704 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences ===
!'Project [unresolvedalias(cast('key as string), None), unresolvedalias(cast('value as string), None)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          'Project [unresolvedalias(cast(key#38 as string), None), unresolvedalias(cast(value#39 as string), None)]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]
           
21/07/22 08:18:47.729 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases ===
!'Project [unresolvedalias(cast(key#38 as string), None), unresolvedalias(cast(value#39 as string), None)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Project [cast(key#38 as string) AS key#58, cast(value#39 as string) AS value#59]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]
           
21/07/22 08:18:47.771 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.ResolveTimeZone ===
 Project [cast(key#38 as string) AS key#58, cast(value#39 as string) AS value#59]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Project [cast(key#38 as string) AS key#58, cast(value#39 as string) AS value#59]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]
           
21/07/22 08:18:47.775 Thread-3 TRACE Analyzer$ResolveReferences: Attempting to resolve Project [cast(key#38 as string) AS key#58, cast(value#39 as string) AS value#59]
21/07/22 08:18:47.783 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Resolution after 2 iterations.
21/07/22 08:18:47.798 Thread-3 TRACE PlanChangeLogger: 
=== Result of Batch Resolution ===
!'Project [unresolvedalias(cast('key as string), None), unresolvedalias(cast('value as string), None)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Project [cast(key#38 as string) AS key#58, cast(value#39 as string) AS value#59]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]
          
21/07/22 08:18:47.799 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Apply Char Padding after 1 iterations.
21/07/22 08:18:47.802 Thread-3 TRACE PlanChangeLogger: Batch Apply Char Padding has no effect.
21/07/22 08:18:47.803 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Post-Hoc Resolution after 1 iterations.
21/07/22 08:18:47.807 Thread-3 TRACE PlanChangeLogger: Batch Post-Hoc Resolution has no effect.
21/07/22 08:18:47.809 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Normalize Alter Table after 1 iterations.
21/07/22 08:18:47.810 Thread-3 TRACE PlanChangeLogger: Batch Normalize Alter Table has no effect.
21/07/22 08:18:47.812 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove Unresolved Hints after 1 iterations.
21/07/22 08:18:47.813 Thread-3 TRACE PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
21/07/22 08:18:47.814 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Nondeterministic after 1 iterations.
21/07/22 08:18:47.815 Thread-3 TRACE PlanChangeLogger: Batch Nondeterministic has no effect.
21/07/22 08:18:47.817 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UDF after 1 iterations.
21/07/22 08:18:47.818 Thread-3 TRACE PlanChangeLogger: Batch UDF has no effect.
21/07/22 08:18:47.819 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UpdateNullability after 1 iterations.
21/07/22 08:18:47.822 Thread-3 TRACE PlanChangeLogger: Batch UpdateNullability has no effect.
21/07/22 08:18:47.823 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Subquery after 1 iterations.
21/07/22 08:18:47.823 Thread-3 TRACE PlanChangeLogger: Batch Subquery has no effect.
21/07/22 08:18:47.833 Thread-3 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.CleanupAliases ===
 Project [cast(key#38 as string) AS key#58, cast(value#39 as string) AS value#59]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Project [cast(key#38 as string) AS key#58, cast(value#39 as string) AS value#59]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]
           
21/07/22 08:18:47.835 Thread-3 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Cleanup after 2 iterations.
21/07/22 08:18:47.843 Thread-3 TRACE PlanChangeLogger: 
=== Result of Batch Cleanup ===
 Project [cast(key#38 as string) AS key#58, cast(value#39 as string) AS value#59]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Project [cast(key#38 as string) AS key#58, cast(value#39 as string) AS value#59]
 +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=changes], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1c37c9da,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, startingOffsets -> earliest, subscribe -> changes),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]
          
21/07/22 08:18:47.850 Thread-3 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 150
Total time: 0.032239546 seconds
Total number of effective runs: 4
Total time of effective runs: 0.013096483 seconds
      
21/07/22 08:18:47.928 Thread-3 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b541a0dd-d329-4e8f-afd5-35886d4e2574. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
21/07/22 08:18:47.941 Thread-3 INFO MicroBatchExecution: Checkpoint root /tmp/temporary-b541a0dd-d329-4e8f-afd5-35886d4e2574 resolved to file:/tmp/temporary-b541a0dd-d329-4e8f-afd5-35886d4e2574.
21/07/22 08:18:47.951 Thread-3 DEBUG UserGroupInformation: PrivilegedAction as:daniel (auth:SIMPLE) from:org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:333)
21/07/22 08:18:48.076 Thread-3 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b541a0dd-d329-4e8f-afd5-35886d4e2574/metadata using temp file file:/tmp/temporary-b541a0dd-d329-4e8f-afd5-35886d4e2574/.metadata.acdb24ca-68d3-4ed9-8446-d37416ee2a72.tmp
21/07/22 08:18:48.243 Thread-3 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b541a0dd-d329-4e8f-afd5-35886d4e2574/.metadata.acdb24ca-68d3-4ed9-8446-d37416ee2a72.tmp to file:/tmp/temporary-b541a0dd-d329-4e8f-afd5-35886d4e2574/metadata
21/07/22 08:18:48.256 Thread-3 DEBUG UserGroupInformation: PrivilegedAction as:daniel (auth:SIMPLE) from:org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:333)
21/07/22 08:18:48.274 Thread-3 DEBUG UserGroupInformation: PrivilegedAction as:daniel (auth:SIMPLE) from:org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:333)
21/07/22 08:18:48.289 Thread-3 INFO MicroBatchExecution: Starting [id = 70d66ac5-0f6b-4181-bb19-480c08f0f5ef, runId = 0b71ac68-8f7f-4ab9-b222-efa079c7362b]. Use file:/tmp/temporary-b541a0dd-d329-4e8f-afd5-35886d4e2574 to store the query checkpoint.
21/07/22 08:18:48.302 stream execution thread for [id = 70d66ac5-0f6b-4181-bb19-480c08f0f5ef, runId = 0b71ac68-8f7f-4ab9-b222-efa079c7362b] INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@187af948] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@6dafd132]
21/07/22 08:18:48.306 stream execution thread for [id = 70d66ac5-0f6b-4181-bb19-480c08f0f5ef, runId = 0b71ac68-8f7f-4ab9-b222-efa079c7362b] ERROR MicroBatchExecution: Query [id = 70d66ac5-0f6b-4181-bb19-480c08f0f5ef, runId = 0b71ac68-8f7f-4ab9-b222-efa079c7362b] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
21/07/22 08:18:48.310 dispatcher-event-loop-1 DEBUG StateStoreCoordinator: Deactivating instances related to checkpoint location 0b71ac68-8f7f-4ab9-b222-efa079c7362b: 
21/07/22 08:19:17.757 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:20:17.754 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:21:17.756 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:22:17.755 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:23:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:24:17.754 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:25:17.754 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:26:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:27:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:28:17.754 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:29:17.754 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:30:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:31:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:32:17.754 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:33:17.754 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:34:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:35:17.755 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:36:17.754 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:37:17.754 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:38:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:39:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:40:17.754 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:41:17.754 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:42:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:43:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:44:17.754 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:45:17.754 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:46:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:47:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:48:17.754 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:49:17.754 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:50:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:51:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:52:17.754 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:53:17.754 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:54:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:55:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:56:17.754 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:57:17.754 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:58:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 08:59:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:00:17.777 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:01:17.758 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:02:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:03:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:04:17.754 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:05:17.754 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:06:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:07:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:08:17.754 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:09:17.754 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:10:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:11:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:12:17.756 dispatcher-event-loop-1 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:13:17.756 dispatcher-event-loop-2 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:14:17.754 dispatcher-event-loop-3 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:15:17.754 dispatcher-event-loop-0 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
21/07/22 09:18:46.837 stream execution thread for [id = 3a357d85-9b12-4b3e-ab4a-4d22a026315e, runId = c0575803-65be-41fe-b6f1-36002a4dccdf] ERROR MicroBatchExecution: Query [id = 3a357d85-9b12-4b3e-ab4a-4d22a026315e, runId = c0575803-65be-41fe-b6f1-36002a4dccdf] terminated with error
java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:579)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan.toMicroBatchStream(KafkaSourceProvider.scala:465)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)
